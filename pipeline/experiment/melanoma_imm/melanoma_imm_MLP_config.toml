# MLPClassifier model configuration file
# Global configuration
[global]
    model = "mlp"
    device = "auto"  # Auto-detect GPU/CPU
    seed = 42
    inputfl = "./datasets/tumor_imm/melanoma_immunotherapy.pkl"
    outputfl = "melanoma_imm_MLP.pkl"
    outdir = "experiment/melanoma_imm_MLP"

# Dataset configuration
[dataset]
    test_size = 0.2
    val_size = 0.2
    random_state = 42

# MLPClassifier model configuration
[mlp]
    # Basic parameters
    hidden_layer_sizes = [[256], [512], [512, 256], [256, 128]]  # Hidden layer structure
    activation = ["relu", "tanh"]  # Activation function
    solver = ["adam", "sgd"]  # Optimizer
    alpha = [0.0001, 0.001, 0.01]  # L2 regularization parameter
    learning_rate = ["constant", "adaptive"]  # Learning rate strategy
    max_iter = 500  # Maximum iterations

    # Other optional parameters
    random_state = 42  # Random seed
    early_stopping = true  # Early stopping
    validation_fraction = 0.1  # Validation set proportion

    # Cross-validation configuration
    cv_folds = 5  # Cross-validation folds
    cv_scoring = "roc_auc"  # Evaluation metric
    n_jobs = 8  # Number of parallel jobs

    # Feature selection configuration - MLP can handle more features, but needs feature selection
    feature_selection = true
    selection_method = "differential_expression"
    n_features = 7000  # MLP can handle more features
    selection_alpha = 0.01

    # Data preprocessing configuration - MLP is sensitive to feature scale, must standardize
    [mlp.preprocessing]
        fill_na = true
        fill_method = "mean"
        scale_data = true  # MLP must be standardized
        scale_method = "standard"

# Training configuration
[training]
    verbose = 1
    save_model = true
    save_format = "pickle"
    save_data = true

# Evaluation configuration
[evaluation]
    metrics = ["roc_auc", "classification_report"]
    save_predictions = true
    save_probabilities = true 