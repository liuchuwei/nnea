[global]
    seed = 888888888
    device = "cuda"
    task = "tumor_immunotherapy" # tumor_sur, cell_class, tumor_drug, tumor_immunotherapy
    model = "NN" # nnea, LR, DT
    train_mod = "cross_validation"  # "one_split, cross_validation"
    n_iter = 500 # iter of random search

[trainer]
    num_epochs = 10000
    batch_size = 64 # [48, 64, 96, 128]
    patience_metric = 1
    patience_loss = 10
    verbose = 0
    n_jobs = 1
    scoring = 'roc_auc' # neg_mean_squared_error

[dataload]
    dataset = 'imm_response'
    test_size = 0.1
    val_size = 0.1
    strategy  = "StratifiedKFold"
    n_splits = 5
    shuffle = true
    scaler = "min_max"
    top_gene = 3000

[LR] #  Logistic_Regression
    max_iter = 5000
    penalty = ["l1", "l2", "elasticnet"]
    solver = ["saga"]
    hyper_C_type = "loguniform"
    hyper_C_min = 1e-3
    hyper_C_max = 1e3
    class_weight = ["balanced"]

[DT] # Decision_Tree
    max_depth = [5, 10, 20]  # null表示无限制
    min_samples_split = [2, 5, 10]
    min_samples_leaf = [1, 2, 4]
    criterion = ["gini", "entropy"]
    class_weight = ["balanced"]

[RF] # Random_Forest
    n_estimators = [50, 100, 200]  # 树的数量
    max_depth = [5, 10, 20]
    min_samples_split = [2, 5]
    max_features = ["sqrt", "log2"]
    class_weight = ["balanced"]

[AB] # Adaptive_Boosting
    n_estimators = [50, 100, 200]  # 弱分类器数量
    learning_rate = [0.01, 0.1, 1.0]  # 学习率
    algorithm = ["SAMME", "SAMME.R"]

[LinearSVM] # Linear_SVM
    max_iter = 5000
    kernel = ["linear"]
    hyper_C_type = "loguniform"
    hyper_C_min = 1e-3
    hyper_C_max = 1e3
    class_weight = ["balanced"]

[RBFSVM] # Linear_SVM
    max_iter = 5000
    kernel = ["rbf"]
    hyper_C_type = "loguniform"
    hyper_C_min = 1e-3
    hyper_C_max = 1e3
    class_weight = ["balanced"]
    gamma_min =  1e-3
    gamma_max = 1e3

[NN] # Neural_Network
    hidden_layer_sizes = [[50], [100], [50, 50]]  # 隐藏层结构
    activation = ["relu", "tanh"]
    solver = ["adam", "sgd"]
    alpha_type = "loguniform"  # L2正则化系数
    alpha_min = 1e-5
    alpha_max = 1e-2
    learning_rate = ["constant", "adaptive"]
    max_iter = 200
    class_weight = ["balanced"]


[nnea]

    [nnea.piror_knowldege]
        use_piror_knowldege = false
        piror_path = "data/genesets/c1.all.v2025.1.Hs.symbols.gmt"
        freeze_prior = false

    [nnea.geneset_layer]
        geneset_layer_mode = "one_mode" # one_mode; deep_mod
        use_piror_knowldege = false
        geneset_layer_alpha = 0.25
        num_sets = 20  # 5~30, step = 1 [5, 30, 1]
        set_min_size = 3
        set_max_size = 20
        num_fc_layers = 0
        geneset_dropout = 0.30

    [nnea.deep_mod]
        geneset_layers = 2
        sub_num_genesets = 10

    [nnea.classifier]
        classifier_name = 'linear' # linear or attention
        hidden_dims = [128, 64, 32]
        output_dim = 2
        classifier_dropout = [0.4, 0.3] # [[0.4, 0.3], [0.3, 0.3], [0.3, 0.25], [0.25, 0.25], [0.25, 0.2], [0.2, 0.2]]
        class_names = [0, 1]

    [nnea.optimizer]
        lr = 0.001 # 0.00025, 0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01
        opt = "adam"
        opt_scheduler = "reduce"
        opt_factor = 0.5
        opt_patience = 5
        weight_decay = 1e-5 # 1e-4, 0.25e-4, 0.75e-4, 1e-5, 0.75e-5, 0.25e-5, 1e-6
