import os
from typing import Dict, Any, Optional, List, Tuple
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import PCA
import seaborn as sns
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from nnea.model.base import BaseModel
import warnings
warnings.filterwarnings('ignore')
import random
import logging

class NeuralUMAP(nn.Module):
    """
    åŸºäºç¥ç»ç½‘ç»œçš„UMAPå®ç°
    
    è¿™ä¸ªå®ç°ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¥å­¦ä¹ é«˜ç»´æ•°æ®åˆ°ä½ç»´ç©ºé—´çš„æ˜ å°„ï¼Œ
    åŒæ—¶ä¿æŒæ•°æ®çš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚
    """
    
    def __init__(self, input_dim: int, embedding_dim: int = 2, 
                 hidden_dims: List[int] = [128, 64, 32], 
                 dropout: float = 0.1):
        """
        åˆå§‹åŒ–ç¥ç»ç½‘ç»œUMAPæ¨¡å‹
        
        Args:
            input_dim: è¾“å…¥æ•°æ®ç»´åº¦
            embedding_dim: åµŒå…¥ç©ºé—´ç»´åº¦ï¼ˆé€šå¸¸ä¸º2ç”¨äºå¯è§†åŒ–ï¼‰
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
            dropout: Dropoutæ¯”ç‡
        """
        super(NeuralUMAP, self).__init__()
        
        self.input_dim = input_dim
        self.embedding_dim = embedding_dim
        self.hidden_dims = hidden_dims
        
        # ç¼–ç å™¨ï¼šé«˜ç»´ -> ä½ç»´
        encoder_layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        encoder_layers.append(nn.Linear(prev_dim, embedding_dim))
        
        self.encoder = nn.Sequential(*encoder_layers)
        
        # è§£ç å™¨ï¼šä½ç»´ -> é«˜ç»´ï¼ˆå¯é€‰ï¼Œç”¨äºé‡æ„ï¼‰
        decoder_layers = []
        prev_dim = embedding_dim
        
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        # é‡æ„è¾“å‡ºå±‚
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        
        self.decoder = nn.Sequential(*decoder_layers)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded
    
    def encode(self, x):
        """ä»…ç¼–ç """
        return self.encoder(x)
    
    def decode(self, z):
        """ä»…è§£ç """
        return self.decoder(z)


class UMAPLoss(nn.Module):
    """
    UMAPæŸå¤±å‡½æ•°å®ç°ï¼Œä½¿ç”¨PCAå¯»æ‰¾æœ€è¿‘é‚»å¯¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    å‚è€ƒnn_umap.pyçš„å®ç°ï¼Œæ·»åŠ ç¼“å­˜æœºåˆ¶æå‡è®­ç»ƒæ•ˆç‡
    """
    
    def __init__(self, min_dist: float = 0.1, a: float = 1.0, b: float = 1.0, 
                 n_neighbors: int = 15, pca_components: int = 50, use_vectorized: bool = True, debug: bool = False):
        """
        åˆå§‹åŒ–UMAPæŸå¤±å‡½æ•°
        
        Args:
            min_dist: æœ€å°è·ç¦»å‚æ•°
            a, b: UMAPçš„aå’Œbå‚æ•°
            n_neighbors: é‚»å±…æ•°é‡
            pca_components: PCAç»„ä»¶æ•°é‡
            use_vectorized: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–å®ç°ï¼ˆæ›´é«˜æ•ˆï¼‰
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        super(UMAPLoss, self).__init__()
        self.min_dist = min_dist
        self.a = a
        self.b = b
        self.n_neighbors = n_neighbors
        self.pca_components = pca_components
        self.use_vectorized = use_vectorized
        self.debug = debug
        self.pca = None
        self.original_data = None
        # æ·»åŠ ç¼“å­˜å˜é‡
        self.positive_pairs = None
        self.negative_pairs = None
        self.is_fitted = False
        # æ·»åŠ å…¨å±€è·ç¦»çŸ©é˜µç¼“å­˜
        self.global_distances = None
        # æ·»åŠ æŸå¤±ç»Ÿè®¡ä¿¡æ¯
        self.loss_stats = {}
        # æ·»åŠ logger
        import logging
        self.logger = logging.getLogger(__name__)
        
    def set_umap_params(self, a: float = None, b: float = None, min_dist: float = None):
        """
        è®¾ç½®UMAPå‚æ•°
        
        Args:
            a: UMAPçš„aå‚æ•°
            b: UMAPçš„bå‚æ•°
            min_dist: æœ€å°è·ç¦»å‚æ•°
        """
        if a is not None:
            self.a = a
        if b is not None:
            self.b = b
        if min_dist is not None:
            self.min_dist = min_dist
        
        self.logger.info(f"UMAPå‚æ•°å·²æ›´æ–°: a={self.a}, b={self.b}, min_dist={self.min_dist}")
        
    def get_loss_stats(self):
        """
        è·å–æŸå¤±å‡½æ•°ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'a': self.a,
            'b': self.b,
            'min_dist': self.min_dist,
            'n_neighbors': self.n_neighbors,
            'pca_components': self.pca_components,
            'is_fitted': self.is_fitted,
            'has_global_distances': hasattr(self, 'global_distances') and self.global_distances is not None
        }
        
        if self.is_fitted:
            stats.update({
                'nbr_indices_shape': self.nbr_indices.shape if self.nbr_indices is not None else None,
                'original_data_shape': self.original_data.shape if self.original_data is not None else None,
                'global_distances_shape': self.global_distances.shape if self.global_distances is not None else None
            })
        
        return stats
        
    def fit_pca(self, X: np.ndarray, nadata=None):
        """
        ä½¿ç”¨PCAæ‹Ÿåˆæ•°æ®å¹¶è¿”å›æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¢å¼•ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
        
        Args:
            X: åŸå§‹é«˜ç»´æ•°æ®
            nadata: nadataå¯¹è±¡ï¼Œå¦‚æœæä¾›ä¸”åŒ…å«é¢„è®¡ç®—çš„PCAæ•°æ®ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
            
        Returns:
            tuple: (pos_indices, neg_indices) æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¢å¼•æ•°ç»„
        """
        if self.is_fitted:
            return self.pos_indices, self.neg_indices
            
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»nadata.unsä¸­è¯»å–é¢„è®¡ç®—çš„PCAæ•°æ®
        X_pca = None
        
        if nadata is not None and hasattr(nadata, 'uns'):
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„PCAæ•°æ®
            if 'pca' in nadata.uns:
                X_pca = nadata.uns['pca']
                self.logger.info("ä»nadata.unsä¸­è¯»å–é¢„è®¡ç®—çš„PCAæ•°æ®")
                
                # ç¡®ä¿PCAæ•°æ®çš„å½¢çŠ¶æ­£ç¡®
                if X_pca.shape[0] != X.shape[0]:
                    self.logger.warning(f"PCAæ•°æ®æ ·æœ¬æ•°({X_pca.shape[0]})ä¸è¾“å…¥æ•°æ®æ ·æœ¬æ•°({X.shape[0]})ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—")
                    X_pca = None
            else:
                self.logger.info("nadata.unsä¸­æœªæ‰¾åˆ°é¢„è®¡ç®—çš„PCAæ•°æ®")
        else:
            self.logger.info("nadataå¯¹è±¡ä¸ºç©ºæˆ–æ²¡æœ‰unså±æ€§")
            
        # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„PCAæ•°æ®ï¼Œåˆ™é‡æ–°è®¡ç®—
        if X_pca is None:
            # é™åˆ¶PCAç»„ä»¶æ•°é‡ä¸è¶…è¿‡ç‰¹å¾æ•°é‡
            n_components = min(self.pca_components, X.shape[1])
            self.pca = PCA(n_components=n_components)
            self.pca.fit(X)
            
            # ä½¿ç”¨PCAé™ç»´
            X_pca = self.pca.transform(X)
            self.logger.info(f"é‡æ–°è®¡ç®—PCAï¼Œç»„ä»¶æ•°: {n_components}")
        else:
            self.logger.info(f"ä½¿ç”¨é¢„è®¡ç®—çš„PCAæ•°æ®ï¼Œå½¢çŠ¶: {X_pca.shape}")
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„PCAå¯¹è±¡ä»¥ä¿æŒå…¼å®¹æ€§
            self.pca = None
        
        # ä½¿ç”¨sklearnçš„NearestNeighborså¯»æ‰¾æœ€è¿‘é‚»
        nbrs = NearestNeighbors(n_neighbors=self.n_neighbors + 1, algorithm='auto').fit(X_pca)
        distances, nbr_indices = nbrs.kneighbors(X_pca)
        
        # è®¡ç®—å¹¶ç¼“å­˜å…¨å±€è·ç¦»çŸ©é˜µï¼ˆç”¨äºè´Ÿæ ·æœ¬é€‰æ‹©ï¼‰
        self.global_distances = distances
        
        # ç”Ÿæˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¢å¼•
        pos_indices, neg_indices = self._generate_pos_neg_pairs(nbr_indices, X.shape[0])
        
        # å°†ç»“æœå­˜å‚¨åˆ°nadata.unsä¸­
        if nadata is not None:
            if not hasattr(nadata, 'uns'):
                nadata.uns = {}
            nadata.uns['pca'] = X_pca
            nadata.uns['nbr_indices'] = nbr_indices
            nadata.uns['pos_indices'] = pos_indices
            nadata.uns['neg_indices'] = neg_indices
            nadata.uns['global_distances'] = self.global_distances
            self.logger.info("å·²å°†PCAã€pos_indicesã€neg_indiceså’Œglobal_distancesæ•°æ®å­˜å‚¨åˆ°nadata.unsä¸­")
        
        # ç¼“å­˜ç»“æœ
        self.nbr_indices = nbr_indices
        self.pos_indices = pos_indices
        self.neg_indices = neg_indices
        self.original_data = X
        self.is_fitted = True
        
        return pos_indices, neg_indices
    
    def find_neighbors_pca(self, X: np.ndarray, nadata=None) -> np.ndarray:
        """
        è·å–å·²ç¼“å­˜çš„è¿‘é‚»ç´¢å¼•ï¼ˆå¦‚æœæœªç¼“å­˜åˆ™å…ˆè®¡ç®—ï¼‰
        
        Args:
            X: åŸå§‹é«˜ç»´æ•°æ®
            nadata: nadataå¯¹è±¡ï¼Œå¯é€‰
            
        Returns:
            nbr_indices: è¿‘é‚»ç´¢å¼•æ•°ç»„
        """
        if not self.is_fitted:
            self.fit_pca(X, nadata)
        
        return self.nbr_indices
    
    def _generate_pos_neg_pairs(self, nbr_indices: np.ndarray, n_samples: int):
        """
        ç”Ÿæˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬å¯¹
        
        Args:
            nbr_indices: è¿‘é‚»ç´¢å¼•æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples, n_neighbors+1)
            n_samples: æ ·æœ¬æ•°é‡
            
        Returns:
            tuple: (pos_indices, neg_indices) æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¢å¼•
        """
        # æ­£æ ·æœ¬ï¼šæ¯ä¸ªæ ·æœ¬ä¸å…¶è¿‘é‚»ï¼ˆæ’é™¤è‡ªèº«ï¼‰
        pos_pairs = []
        for i in range(n_samples):
            # è·å–å½“å‰æ ·æœ¬çš„è¿‘é‚»ï¼ˆæ’é™¤è‡ªèº«ï¼‰
            neighbors = nbr_indices[i][1:]  # æ’é™¤ç¬¬ä¸€ä¸ªï¼ˆè‡ªèº«ï¼‰
            for neighbor in neighbors:
                pos_pairs.append([i, neighbor])
        
        pos_indices = np.array(pos_pairs)
        
        # è´Ÿæ ·æœ¬ï¼šéšæœºé€‰æ‹©éè¿‘é‚»çš„æ ·æœ¬å¯¹
        neg_pairs = []
        n_neg_per_sample = min(self.n_neighbors, n_samples - self.n_neighbors - 1)  # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ ·æœ¬æ•°
        
        for i in range(n_samples):
            # è·å–å½“å‰æ ·æœ¬çš„è¿‘é‚»
            neighbors = set(nbr_indices[i])
            
            # éšæœºé€‰æ‹©éè¿‘é‚»çš„æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
            non_neighbors = [j for j in range(n_samples) if j not in neighbors and j != i]
            
            if len(non_neighbors) > 0:
                # éšæœºé€‰æ‹©è´Ÿæ ·æœ¬
                n_neg = min(n_neg_per_sample, len(non_neighbors))
                selected_neg = np.random.choice(non_neighbors, size=n_neg, replace=False)
                
                for neg_idx in selected_neg:
                    neg_pairs.append([i, neg_idx])
        
        neg_indices = np.array(neg_pairs) if neg_pairs else np.empty((0, 2), dtype=int)
        
        self.logger.info(f"ç”Ÿæˆæ ·æœ¬å¯¹: æ­£æ ·æœ¬ {len(pos_indices)} å¯¹, è´Ÿæ ·æœ¬ {len(neg_indices)} å¯¹")
        
        return pos_indices, neg_indices

    def forward(self, embeddings, original_data=None, nadata=None, batch_pos_indices=None, batch_neg_indices=None):
        """
        è®¡ç®—UMAPæŸå¤±
        
        Args:
            embeddings: åµŒå…¥å‘é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, embedding_dim)
            original_data: åŸå§‹æ•°æ®ï¼Œç”¨äºè®¡ç®—é‡æ„æŸå¤±
            nadata: nadataå¯¹è±¡
            batch_pos_indices: æ‰¹æ¬¡æ­£æ ·æœ¬ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º(batch_size, n_pos_pairs, 2)
            batch_neg_indices: æ‰¹æ¬¡è´Ÿæ ·æœ¬ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º(batch_size, n_neg_pairs, 2)
            
        Returns:
            total_loss: æ€»æŸå¤±
        """
        if batch_pos_indices is None or batch_neg_indices is None:
            self.logger.warning("æœªæä¾›æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬ç´¢å¼•ï¼Œè¿”å›é›¶æŸå¤±")
            return torch.tensor(0.0, device=embeddings.device)
        
        # éªŒè¯è¾“å…¥å½¢çŠ¶
        batch_size = embeddings.shape[0]
        if batch_pos_indices.shape[0] != batch_size or batch_neg_indices.shape[0] != batch_size:
            self.logger.error(f"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: embeddings={batch_size}, pos_indices={batch_pos_indices.shape[0]}, neg_indices={batch_neg_indices.shape[0]}")
            return torch.tensor(0.0, device=embeddings.device)
        
        # ç¡®ä¿ç´¢å¼•æ˜¯torchå¼ é‡å¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if not torch.is_tensor(batch_pos_indices):
            batch_pos_indices = torch.tensor(batch_pos_indices, dtype=torch.long, device=embeddings.device)
        elif batch_pos_indices.device != embeddings.device:
            batch_pos_indices = batch_pos_indices.to(embeddings.device)
            
        if not torch.is_tensor(batch_neg_indices):
            batch_neg_indices = torch.tensor(batch_neg_indices, dtype=torch.long, device=embeddings.device)
        elif batch_neg_indices.device != embeddings.device:
            batch_neg_indices = batch_neg_indices.to(embeddings.device)
        
        # é€‰æ‹©ä½¿ç”¨å‘é‡åŒ–å®ç°è¿˜æ˜¯å¾ªç¯å®ç°
        if self.use_vectorized:
            # ä½¿ç”¨å‘é‡åŒ–å®ç°ï¼ˆæ›´é«˜æ•ˆï¼‰
            pos_loss, neg_loss = self._compute_loss_vectorized(embeddings, batch_pos_indices, batch_neg_indices)
        else:
            # ä½¿ç”¨å¾ªç¯å®ç°ï¼ˆæ›´ç›´è§‚ï¼‰
            pos_loss = self._compute_positive_loss(embeddings, batch_pos_indices)
            neg_loss = self._compute_negative_loss(embeddings, batch_neg_indices)
        
        # æ€»æŸå¤±
        total_loss = pos_loss + neg_loss
        
        # è®°å½•æŸå¤±ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self, 'loss_stats'):
            self.loss_stats['pos_loss'] = pos_loss.item()
            self.loss_stats['neg_loss'] = neg_loss.item()
            self.loss_stats['total_loss'] = total_loss.item()
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨éœ€è¦æ—¶æ˜¾ç¤ºï¼‰
        if self.debug:
            self.logger.info(f"UMAPæŸå¤± - æ­£æ ·æœ¬: {pos_loss.item():.6f}, è´Ÿæ ·æœ¬: {neg_loss.item():.6f}, æ€»è®¡: {total_loss.item():.6f}")
        
        return total_loss
    
    def _compute_positive_loss(self, embeddings, pos_indices):
        """
        è®¡ç®—æ­£æ ·æœ¬æŸå¤±
        
        Args:
            embeddings: åµŒå…¥å‘é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, embedding_dim)
            pos_indices: æ­£æ ·æœ¬ç´¢å¼•å¯¹ï¼Œå½¢çŠ¶ä¸º(batch_size, n_pos_pairs, 2)
            
        Returns:
            pos_loss: æ­£æ ·æœ¬æŸå¤±
        """
        if pos_indices.numel() == 0:
            return torch.tensor(0.0, device=embeddings.device)
        
        # å¤„ç†æ‰¹æ¬¡æ•°æ®
        batch_size = embeddings.shape[0]
        pos_pairs = []
        
        # éå†æ¯ä¸ªæ ·æœ¬çš„æ­£æ ·æœ¬å¯¹
        for i in range(batch_size):
            sample_pos_pairs = pos_indices[i]  # å½¢çŠ¶ä¸º(n_pos_pairs, 2)
            for pair in sample_pos_pairs:
                idx1, idx2 = pair[0].item(), pair[1].item()
                # ç¡®ä¿ç´¢å¼•åœ¨æ‰¹æ¬¡èŒƒå›´å†…
                if 0 <= idx1 < batch_size and 0 <= idx2 < batch_size:
                    pos_pairs.append([embeddings[idx1], embeddings[idx2]])
        
        if not pos_pairs:
            return torch.tensor(0.0, device=embeddings.device)
        
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼å †å å¼ é‡
        pos_pairs = torch.stack([torch.stack(pair) for pair in pos_pairs])
        
        # è®¡ç®—æ­£æ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»
        pos_distances = torch.norm(pos_pairs[:, 0] - pos_pairs[:, 1], dim=1)
        
        # UMAPæ­£æ ·æœ¬æŸå¤±ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±
        # ç›®æ ‡ï¼šæ­£æ ·æœ¬å¯¹åº”è¯¥æ¥è¿‘
        pos_targets = torch.ones(len(pos_distances), device=embeddings.device)
        
        # ä½¿ç”¨sigmoidå°†è·ç¦»è½¬æ¢ä¸ºæ¦‚ç‡
        pos_probs = torch.sigmoid(-pos_distances / self.min_dist)
        pos_loss = F.binary_cross_entropy(pos_probs, pos_targets)
        
        return pos_loss
    
    def _compute_negative_loss(self, embeddings, neg_indices):
        """
        è®¡ç®—è´Ÿæ ·æœ¬æŸå¤±
        
        Args:
            embeddings: åµŒå…¥å‘é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, embedding_dim)
            neg_indices: è´Ÿæ ·æœ¬ç´¢å¼•å¯¹ï¼Œå½¢çŠ¶ä¸º(batch_size, n_neg_pairs, 2)
            
        Returns:
            neg_loss: è´Ÿæ ·æœ¬æŸå¤±
        """
        if neg_indices.numel() == 0:
            return torch.tensor(0.0, device=embeddings.device)
        
        # å¤„ç†æ‰¹æ¬¡æ•°æ®
        batch_size = embeddings.shape[0]
        neg_pairs = []
        
        # éå†æ¯ä¸ªæ ·æœ¬çš„è´Ÿæ ·æœ¬å¯¹
        for i in range(batch_size):
            sample_neg_pairs = neg_indices[i]  # å½¢çŠ¶ä¸º(n_neg_pairs, 2)
            for pair in sample_neg_pairs:
                idx1, idx2 = pair[0].item(), pair[1].item()
                # ç¡®ä¿ç´¢å¼•åœ¨æ‰¹æ¬¡èŒƒå›´å†…
                if 0 <= idx1 < batch_size and 0 <= idx2 < batch_size:
                    neg_pairs.append([embeddings[idx1], embeddings[idx2]])
        
        if not neg_pairs:
            return torch.tensor(0.0, device=embeddings.device)
        
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹å¼å †å å¼ é‡
        neg_pairs = torch.stack([torch.stack(pair) for pair in neg_pairs])
        
        # è®¡ç®—è´Ÿæ ·æœ¬å¯¹ä¹‹é—´çš„è·ç¦»
        neg_distances = torch.norm(neg_pairs[:, 0] - neg_pairs[:, 1], dim=1)
        
        # UMAPè´Ÿæ ·æœ¬æŸå¤±ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±
        # ç›®æ ‡ï¼šè´Ÿæ ·æœ¬å¯¹åº”è¯¥è¿œç¦»
        neg_targets = torch.zeros(len(neg_distances), device=embeddings.device)
        
        # ä½¿ç”¨sigmoidå°†è·ç¦»è½¬æ¢ä¸ºæ¦‚ç‡
        neg_probs = torch.sigmoid(-neg_distances / self.min_dist)
        neg_loss = F.binary_cross_entropy(neg_probs, neg_targets)
        
        return neg_loss
    
    def _compute_loss_vectorized(self, embeddings, pos_indices, neg_indices):
        """
        å‘é‡åŒ–è®¡ç®—UMAPæŸå¤±ï¼ˆæ›´é«˜æ•ˆçš„å®ç°ï¼‰
        
        Args:
            embeddings: åµŒå…¥å‘é‡ï¼Œå½¢çŠ¶ä¸º(batch_size, embedding_dim)
            pos_indices: æ­£æ ·æœ¬ç´¢å¼•å¯¹ï¼Œå½¢çŠ¶ä¸º(batch_size, n_pos_pairs, 2)
            neg_indices: è´Ÿæ ·æœ¬ç´¢å¼•å¯¹ï¼Œå½¢çŠ¶ä¸º(batch_size, n_neg_pairs, 2)
            
        Returns:
            pos_loss, neg_loss: æ­£æ ·æœ¬æŸå¤±å’Œè´Ÿæ ·æœ¬æŸå¤±
        """
        batch_size = embeddings.shape[0]
        
        # å‘é‡åŒ–å¤„ç†æ­£æ ·æœ¬å¯¹
        pos_loss = torch.tensor(0.0, device=embeddings.device)
        if pos_indices.numel() > 0:
            # é‡å¡‘ç´¢å¼•ä»¥ä¾¿å‘é‡åŒ–å¤„ç†
            pos_indices_flat = pos_indices.view(-1, 2)
            
            # è¿‡æ»¤æœ‰æ•ˆçš„ç´¢å¼•å¯¹ï¼ˆåœ¨æ‰¹æ¬¡èŒƒå›´å†…ï¼‰
            valid_mask = (pos_indices_flat[:, 0] >= 0) & (pos_indices_flat[:, 0] < batch_size) & \
                        (pos_indices_flat[:, 1] >= 0) & (pos_indices_flat[:, 1] < batch_size)
            
            if valid_mask.any():
                valid_pos_indices = pos_indices_flat[valid_mask]
                pos_embeddings1 = embeddings[valid_pos_indices[:, 0]]
                pos_embeddings2 = embeddings[valid_pos_indices[:, 1]]
                
                # è®¡ç®—è·ç¦»
                pos_distances = torch.norm(pos_embeddings1 - pos_embeddings2, dim=1)
                
                # è®¡ç®—æŸå¤±
                pos_targets = torch.ones(len(pos_distances), device=embeddings.device)
                pos_probs = torch.sigmoid(-pos_distances / self.min_dist)
                pos_loss = F.binary_cross_entropy(pos_probs, pos_targets)
        
        # å‘é‡åŒ–å¤„ç†è´Ÿæ ·æœ¬å¯¹
        neg_loss = torch.tensor(0.0, device=embeddings.device)
        if neg_indices.numel() > 0:
            # é‡å¡‘ç´¢å¼•ä»¥ä¾¿å‘é‡åŒ–å¤„ç†
            neg_indices_flat = neg_indices.view(-1, 2)
            
            # è¿‡æ»¤æœ‰æ•ˆçš„ç´¢å¼•å¯¹ï¼ˆåœ¨æ‰¹æ¬¡èŒƒå›´å†…ï¼‰
            valid_mask = (neg_indices_flat[:, 0] >= 0) & (neg_indices_flat[:, 0] < batch_size) & \
                        (neg_indices_flat[:, 1] >= 0) & (neg_indices_flat[:, 1] < batch_size)
            
            if valid_mask.any():
                valid_neg_indices = neg_indices_flat[valid_mask]
                neg_embeddings1 = embeddings[valid_neg_indices[:, 0]]
                neg_embeddings2 = embeddings[valid_neg_indices[:, 1]]
                
                # è®¡ç®—è·ç¦»
                neg_distances = torch.norm(neg_embeddings1 - neg_embeddings2, dim=1)
                
                # è®¡ç®—æŸå¤±
                neg_targets = torch.zeros(len(neg_distances), device=embeddings.device)
                neg_probs = torch.sigmoid(-neg_distances / self.min_dist)
                neg_loss = F.binary_cross_entropy(neg_probs, neg_targets)
        
        return pos_loss, neg_loss


class NNEAUMAP(BaseModel):
    """
    NNEA UMAPé™ç»´å™¨
    å®ç°åŸºäºç¥ç»ç½‘ç»œçš„UMAPé™ç»´ï¼Œæä¾›å¯è§£é‡Šçš„é™ç»´ç»“æœ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–NNEA UMAPé™ç»´å™¨
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        super().__init__(config)
        self.task = 'umap'

    def build(self, nadata) -> None:
        """
        æ„å»ºæ¨¡å‹
        
        Args:
            nadata: nadataå¯¹è±¡
        """
        if nadata is None:
            raise ValueError("nadataå¯¹è±¡ä¸èƒ½ä¸ºç©º")
        
        # è·å–è¾“å…¥ç»´åº¦
        if hasattr(nadata, 'X') and nadata.X is not None:
            input_dim = nadata.X.shape[1]  # åŸºå› æ•°é‡
        else:
            raise ValueError("è¡¨è¾¾çŸ©é˜µæœªåŠ è½½")
        
        # è·å–UMAPé…ç½®
        umap_config = self.config.get('umap', {})
        embedding_dim = umap_config.get('embedding_dim', 2)
        hidden_dims = umap_config.get('hidden_dims', [128, 64, 32])
        dropout = umap_config.get('dropout', 0.1)
        
        # æ›´æ–°é…ç½®
        self.config['input_dim'] = input_dim
        self.config['embedding_dim'] = embedding_dim
        self.config['device'] = str(self.device)
        
        # åˆ›å»ºæ¨¡å‹
        self.model = NeuralUMAP(
            input_dim=input_dim,
            embedding_dim=embedding_dim,
            hidden_dims=hidden_dims,
            dropout=dropout
        )
        self.model.to(self.device)
        
        # åˆ›å»ºUMAPæŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„PCAç‰ˆæœ¬ï¼‰
        n_neighbors = umap_config.get('n_neighbors', 15)
        min_dist = umap_config.get('min_dist', 0.1)
        pca_components = umap_config.get('pca_components', 50)
        
        self.umap_loss = UMAPLoss(
            min_dist=min_dist,
            n_neighbors=n_neighbors,
            pca_components=pca_components
        ).to(self.device)
        
        self.logger.info(f"NNEA UMAPé™ç»´å™¨å·²æ„å»º: è¾“å…¥ç»´åº¦={input_dim}, åµŒå…¥ç»´åº¦={embedding_dim}")
        self.logger.info(f"éšè—å±‚ç»´åº¦: {hidden_dims}")
        self.logger.info(f"UMAPå‚æ•°: n_neighbors={n_neighbors}, min_dist={min_dist}, pca_components={pca_components}")
    
    def train(self, nadata, verbose: int = 1, max_epochs: Optional[int] = None, **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            nadata: nadataå¯¹è±¡
            verbose: è¯¦ç»†ç¨‹åº¦
                0=åªæ˜¾ç¤ºè¿›åº¦æ¡
                1=æ˜¾ç¤ºè®­ç»ƒæŸå¤±
                2=æ˜¾ç¤ºè®­ç»ƒæŸå¤±å’Œé‡æ„æŸå¤±
            max_epochs: æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„epochs
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªæ„å»º")
        
        # å‡†å¤‡æ•°æ®
        X = nadata.X

        # åœ¨è®­ç»ƒå¼€å§‹å‰å®ŒæˆPCAå’Œè¿‘é‚»è®¡ç®—ï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
        self.logger.info("æ­£åœ¨è®¡ç®—PCAå’Œè¿‘é‚»å…³ç³»...")
        pos_indices, neg_indices = self.umap_loss.fit_pca(X, nadata)
        self.logger.info("PCAå’Œè¿‘é‚»è®¡ç®—å®Œæˆï¼")
        
        # è®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})
        if max_epochs is None:
            epochs = training_config.get('epochs', 100)
        else:
            epochs = max_epochs
        learning_rate = training_config.get('learning_rate', 0.001)
        batch_size = training_config.get('batch_size', 32)
        test_size = training_config.get('test_size', 0.2)
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_tensor = torch.FloatTensor(X)
        
        # è‡ªå®šä¹‰æ•°æ®é›†ç±»
        class UMAPDataset(torch.utils.data.Dataset):
            def __init__(self, X, pos_indices, neg_indices):
                self.X = X
                self.pos_indices = pos_indices
                self.neg_indices = neg_indices
                
                # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºç´¢å¼•æ˜ å°„
                self.sample_to_pos = {}
                self.sample_to_neg = {}
                
                # æ„å»ºæ ·æœ¬åˆ°æ­£æ ·æœ¬å¯¹çš„æ˜ å°„
                for i, (idx1, idx2) in enumerate(pos_indices):
                    if idx1 not in self.sample_to_pos:
                        self.sample_to_pos[idx1] = []
                    self.sample_to_pos[idx1].append(idx2)
                    
                    if idx2 not in self.sample_to_pos:
                        self.sample_to_pos[idx2] = []
                    self.sample_to_pos[idx2].append(idx1)
                
                # æ„å»ºæ ·æœ¬åˆ°è´Ÿæ ·æœ¬å¯¹çš„æ˜ å°„
                for i, (idx1, idx2) in enumerate(neg_indices):
                    if idx1 not in self.sample_to_neg:
                        self.sample_to_neg[idx1] = []
                    self.sample_to_neg[idx1].append(idx2)
                    
                    if idx2 not in self.sample_to_neg:
                        self.sample_to_neg[idx2] = []
                    self.sample_to_neg[idx2].append(idx1)
                
                # è®¡ç®—æœ€å¤§æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬æ•°é‡ï¼Œç”¨äºå¡«å……
                self.max_pos_pairs = 0
                self.max_neg_pairs = 0
                for i in range(len(X)):
                    pos_count = len(self.sample_to_pos.get(i, []))
                    neg_count = len(self.sample_to_neg.get(i, []))
                    self.max_pos_pairs = max(self.max_pos_pairs, pos_count)
                    self.max_neg_pairs = max(self.max_neg_pairs, neg_count)
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬å¯¹
                self.max_pos_pairs = max(self.max_pos_pairs, 1)
                self.max_neg_pairs = max(self.max_neg_pairs, 1)
                
                # è®°å½•æœ€å¤§æ ·æœ¬å¯¹æ•°é‡
                print(f"æ•°æ®é›†ç»Ÿè®¡: æœ€å¤§æ­£æ ·æœ¬å¯¹æ•°é‡={self.max_pos_pairs}, æœ€å¤§è´Ÿæ ·æœ¬å¯¹æ•°é‡={self.max_neg_pairs}")
            
            def __len__(self):
                return len(self.X)
            
            def __getitem__(self, idx):
                # è·å–å½“å‰æ ·æœ¬çš„æ­£æ ·æœ¬ç´¢å¼•
                pos_neighbors = self.sample_to_pos.get(idx, [])
                if len(pos_neighbors) == 0:
                    pos_neighbors = [idx]  # å¦‚æœæ²¡æœ‰æ­£æ ·æœ¬ï¼Œä½¿ç”¨è‡ªèº«
                
                # è·å–å½“å‰æ ·æœ¬çš„è´Ÿæ ·æœ¬ç´¢å¼•
                neg_neighbors = self.sample_to_neg.get(idx, [])
                if len(neg_neighbors) == 0:
                    neg_neighbors = [idx]  # å¦‚æœæ²¡æœ‰è´Ÿæ ·æœ¬ï¼Œä½¿ç”¨è‡ªèº«
                
                # åˆ›å»ºæ­£æ ·æœ¬å¯¹ç´¢å¼•å¹¶å¡«å……åˆ°å›ºå®šå¤§å°
                pos_pairs = [[idx, neighbor] for neighbor in pos_neighbors]
                while len(pos_pairs) < self.max_pos_pairs:
                    pos_pairs.append([idx, idx])  # ç”¨è‡ªèº«å¡«å……
                
                # åˆ›å»ºè´Ÿæ ·æœ¬å¯¹ç´¢å¼•å¹¶å¡«å……åˆ°å›ºå®šå¤§å°
                neg_pairs = [[idx, neighbor] for neighbor in neg_neighbors]
                while len(neg_pairs) < self.max_neg_pairs:
                    neg_pairs.append([idx, idx])  # ç”¨è‡ªèº«å¡«å……
                
                # å°†åŸå§‹ç´¢å¼•è½¬æ¢ä¸ºæ‰¹æ¬¡å†…ç´¢å¼•ï¼ˆç›¸å¯¹ä½ç½®ï¼‰
                # è¿™é‡Œæˆ‘ä»¬è¿”å›åŸå§‹ç´¢å¼•ï¼Œåœ¨DataLoaderçš„collate_fnä¸­è¿›è¡Œè½¬æ¢
                return (self.X[idx], 
                       torch.tensor(pos_pairs, dtype=torch.long),
                       torch.tensor(neg_pairs, dtype=torch.long))
        
        # æ„å»ºå®Œæ•´æ•°æ®é›†
        full_dataset = UMAPDataset(X_tensor, pos_indices, neg_indices)
        
        # ä½¿ç”¨random_splitåˆ†å‰²æ•°æ®é›†
        n_samples = X.shape[0]
        train_size = int(n_samples * (1 - test_size))
        test_size_split = n_samples - train_size
        
        train_dataset, test_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, test_size_split]
        )
        
        self.logger.info(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›† {len(train_dataset)} æ ·æœ¬, æµ‹è¯•é›† {len(test_dataset)} æ ·æœ¬")
        
        # å­˜å‚¨è®­ç»ƒé›†ç´¢å¼•ä¾›åç»­ä½¿ç”¨ï¼ˆä»random_splitè·å–ï¼‰
        self.train_indices = train_dataset.indices
        
        # å®šä¹‰collateå‡½æ•°ï¼Œå°†åŸå§‹ç´¢å¼•è½¬æ¢ä¸ºæ‰¹æ¬¡å†…ç´¢å¼•
        def umap_collate_fn(batch):
            """
            å°†æ‰¹æ¬¡æ•°æ®ä¸­çš„åŸå§‹ç´¢å¼•è½¬æ¢ä¸ºæ‰¹æ¬¡å†…ç´¢å¼•
            """
            batch_X = []
            batch_pos_indices = []
            batch_neg_indices = []
            
            # è·å–å½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†ä¸­çš„ç´¢å¼•
            # ç”±äºrandom_splitä¼šé‡æ–°ç´¢å¼•ï¼Œæˆ‘ä»¬éœ€è¦ä»train_dataset.indicesè·å–åŸå§‹ç´¢å¼•
            batch_original_indices = [train_dataset.indices[i] for i in range(len(batch))]
            
            # åˆ›å»ºåŸå§‹ç´¢å¼•åˆ°æ‰¹æ¬¡å†…ç´¢å¼•çš„æ˜ å°„
            original_to_batch = {orig_idx: batch_idx for batch_idx, orig_idx in enumerate(batch_original_indices)}
            
            for i, (X_item, pos_pairs, neg_pairs) in enumerate(batch):
                batch_X.append(X_item)
                
                # å°†åŸå§‹ç´¢å¼•è½¬æ¢ä¸ºæ‰¹æ¬¡å†…ç´¢å¼•
                pos_pairs_batch = pos_pairs.clone()
                neg_pairs_batch = neg_pairs.clone()
                
                # è½¬æ¢æ­£æ ·æœ¬å¯¹ç´¢å¼•
                for j in range(pos_pairs_batch.shape[0]):
                    orig_idx1, orig_idx2 = pos_pairs_batch[j]
                    if orig_idx1 in original_to_batch and orig_idx2 in original_to_batch:
                        pos_pairs_batch[j, 0] = original_to_batch[orig_idx1]
                        pos_pairs_batch[j, 1] = original_to_batch[orig_idx2]
                    else:
                        # å¦‚æœç´¢å¼•ä¸åœ¨å½“å‰æ‰¹æ¬¡ä¸­ï¼Œä½¿ç”¨è‡ªèº«ç´¢å¼•
                        pos_pairs_batch[j, 0] = i
                        pos_pairs_batch[j, 1] = i
                
                # è½¬æ¢è´Ÿæ ·æœ¬å¯¹ç´¢å¼•
                for j in range(neg_pairs_batch.shape[0]):
                    orig_idx1, orig_idx2 = neg_pairs_batch[j]
                    if orig_idx1 in original_to_batch and orig_idx2 in original_to_batch:
                        neg_pairs_batch[j, 0] = original_to_batch[orig_idx1]
                        neg_pairs_batch[j, 1] = original_to_batch[orig_idx2]
                    else:
                        # å¦‚æœç´¢å¼•ä¸åœ¨å½“å‰æ‰¹æ¬¡ä¸­ï¼Œä½¿ç”¨è‡ªèº«ç´¢å¼•
                        neg_pairs_batch[j, 0] = i
                        neg_pairs_batch[j, 1] = i
                
                batch_pos_indices.append(pos_pairs_batch)
                batch_neg_indices.append(neg_pairs_batch)
            
            # å †å æ‰¹æ¬¡æ•°æ®
            batch_X = torch.stack(batch_X)
            batch_pos_indices = torch.stack(batch_pos_indices)
            batch_neg_indices = torch.stack(batch_neg_indices)
            
            return batch_X, batch_pos_indices, batch_neg_indices
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            collate_fn=umap_collate_fn
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        
        # æ—©åœæœºåˆ¶å‚æ•°
        patience = training_config.get('patience', 10)
        min_delta = 1e-6
        
        # æ—©åœå˜é‡åˆå§‹åŒ–
        best_loss = float('inf')
        patience_counter = 0
        early_stopped = False
        
        # è®­ç»ƒå¾ªç¯
        train_losses = []
        
        if verbose >= 1:
            self.logger.info("å¼€å§‹è®­ç»ƒNNEA UMAPæ¨¡å‹...")
            self.logger.info(f"æ—©åœé…ç½®: patience={patience}, min_delta={min_delta}")
        
        # å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡
        try:
            from tqdm import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåªæœ‰verbose=0æ—¶æ˜¾ç¤ºï¼‰
        if verbose == 0 and use_tqdm:
            pbar = tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦")
        else:
            pbar = range(epochs)
        
        for epoch in pbar:
            # è®­ç»ƒæ¨¡å¼
            self.model.train()
            epoch_loss = 0.0
            num_batches = 0
            
            # ä½¿ç”¨æ•°æ®åŠ è½½å™¨è¿›è¡Œæ‰¹å¤„ç†è®­ç»ƒ
            for batch_idx, (batch_X, batch_pos_indices, batch_neg_indices) in enumerate(train_loader):

                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch_X = batch_X.to(self.device)
                
                optimizer.zero_grad()
                
                try:
                    # å‰å‘ä¼ æ’­
                    encoded, decoded = self.model(batch_X)

                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºindicesæ•°é‡
                    if verbose >= 2 and batch_idx == 0:
                        self.logger.info(f"Epoch {epoch}, Batch {batch_idx}: batch_pos_indices={batch_pos_indices.shape}, batch_neg_indices={batch_neg_indices.shape}")


                    umap_loss = self.umap_loss(encoded, X, nadata, batch_pos_indices, batch_neg_indices)
                    
                    # è®¡ç®—é‡æ„æŸå¤±ï¼ˆå¯é€‰ï¼‰
                    recon_loss = F.mse_loss(decoded, batch_X)
                    
                    # æ€»æŸå¤±
                    total_loss = umap_loss + 0.1 * recon_loss
                    
                    # åå‘ä¼ æ’­
                    total_loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_loss += total_loss.item()
                    num_batches += 1
                    
                except Exception as e:
                    self.logger.error(f"Epoch {epoch}, Batch {batch_idx}: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    continue
            
            # è®¡ç®—å¹³å‡æŸå¤±
            if num_batches > 0:
                avg_loss = epoch_loss / num_batches
                train_losses.append(avg_loss)
                
                # verbose=1æ—¶æ˜¾ç¤ºè®­ç»ƒæŸå¤±
                if verbose >= 1:
                    self.logger.info(f"Epoch {epoch}: Train Loss={avg_loss:.4f}")
                
                # æ—©åœæ£€æŸ¥
                if avg_loss < best_loss - min_delta:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ
                if patience_counter >= patience:
                    early_stopped = True
                    self.logger.info(f"ğŸ›‘ Epoch {epoch}: è§¦å‘æ—©åœï¼æŸå¤±åœ¨{patience}ä¸ªepochå†…æœªæ”¹å–„")
                    break
        
        # è®­ç»ƒå®Œæˆ
        self.is_trained = True
        
        # è®°å½•æ—©åœä¿¡æ¯
        if early_stopped:
            self.logger.info(f"ğŸ“Š è®­ç»ƒå› æ—©åœè€Œç»“æŸï¼Œå®é™…è®­ç»ƒäº†{epoch+1}ä¸ªepoch")
        else:
            self.logger.info(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒäº†{epochs}ä¸ªepoch")
        
        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        cache_info = self.get_cache_info()
        self.logger.info("ç¼“å­˜ä¿¡æ¯ï¼š")
        self.logger.info(f"- PCAå·²æ‹Ÿåˆ: {cache_info['pca_fitted']}")
        self.logger.info(f"- è¿‘é‚»ç´¢å¼•å½¢çŠ¶: {cache_info['nbr_indices_shape']}")
        self.logger.info(f"- PCAç»„ä»¶æ•°: {cache_info['pca_components']}")
        self.logger.info(f"- åŸå§‹æ•°æ®å½¢çŠ¶: {cache_info['original_data_shape']}")
        self.logger.info(f"- å…¨å±€è·ç¦»çŸ©é˜µå½¢çŠ¶: {cache_info['global_distances_shape']}")
        self.logger.info(f"- æ™ºèƒ½è´Ÿæ ·æœ¬é‡‡æ ·: {cache_info['smart_negative_sampling']}")
        self.logger.info("âœ… æ”¹è¿›ï¼šä½¿ç”¨å…¨å±€è·ç¦»ä¿¡æ¯è¿›è¡Œæ™ºèƒ½è´Ÿæ ·æœ¬é€‰æ‹©ï¼Œæé«˜UMAPè´¨é‡")
        
        # è¿”å›è®­ç»ƒç»“æœ
        results = {
            'train_losses': train_losses,
            'final_train_loss': train_losses[-1] if train_losses else None,
            'epochs_trained': epoch + 1 if early_stopped else epochs,
            'early_stopped': early_stopped,
            'best_loss': best_loss
        }
        
        return results

    def predict(self, nadata) -> np.ndarray:
        """
        æ¨¡å‹é¢„æµ‹ï¼ˆé™ç»´ï¼‰
        
        Args:
            nadata: nadataå¯¹è±¡
            
        Returns:
            é™ç»´åçš„åµŒå…¥ç»“æœ
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        self.model.eval()
        with torch.no_grad():
            X = nadata.X
            
            # æ•°æ®æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            encoded, _ = self.model(X_tensor)
            return encoded.cpu().numpy()
    
    def evaluate(self, nadata, split='test') -> Dict[str, float]:
        """
        æ¨¡å‹è¯„ä¼°
        
        Args:
            nadata: nadataå¯¹è±¡
            split: è¯„ä¼°çš„æ•°æ®é›†åˆ†å‰²
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # è·å–æ•°æ®ç´¢å¼•
        indices = nadata.Model.get_indices(split)
        if indices is None:
            raise ValueError(f"æœªæ‰¾åˆ°{split}é›†çš„ç´¢å¼•")
        
        # æ ¹æ®ç´¢å¼•è·å–æ•°æ®
        X = nadata.X[indices]
        
        # è·å–åµŒå…¥ç»“æœ
        embeddings = self.predict(nadata)
        embeddings_split = embeddings[indices]
        
        # è®¡ç®—é™ç»´è´¨é‡æŒ‡æ ‡
        try:
            # é‡æ„è¯¯å·®
            X_tensor = torch.FloatTensor(X).to(self.device)
            with torch.no_grad():
                encoded, decoded = self.model(X_tensor)
                reconstruction_error = F.mse_loss(decoded, X_tensor).item()
            
            # å¦‚æœæœ‰å…³è”çš„æ ‡ç­¾ï¼Œè®¡ç®—èšç±»æŒ‡æ ‡
            if hasattr(nadata, 'Meta') and nadata.Meta is not None:
                target_col = self.config.get('dataset', {}).get('target_column', 'target')
                if target_col in nadata.Meta.columns:
                    labels = nadata.Meta.iloc[indices][target_col].values
                    
                    # è®¡ç®—èšç±»æŒ‡æ ‡
                    silhouette = silhouette_score(embeddings_split, labels)
                    calinski_harabasz = calinski_harabasz_score(embeddings_split, labels)
                    davies_bouldin = davies_bouldin_score(embeddings_split, labels)
                    
                    results = {
                        'reconstruction_error': reconstruction_error,
                        'silhouette_score': silhouette,
                        'calinski_harabasz_score': calinski_harabasz,
                        'davies_bouldin_score': davies_bouldin
                    }
                else:
                    results = {
                        'reconstruction_error': reconstruction_error
                    }
            else:
                results = {
                    'reconstruction_error': reconstruction_error
                }
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—è¯„ä¼°æŒ‡æ ‡æ—¶å‡ºç°é”™è¯¯: {e}")
            results = {
                'reconstruction_error': float('inf')
            }
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°Modelå®¹å™¨
        eval_results = nadata.Model.get_metadata('evaluation_results') or {}
        eval_results[split] = results
        nadata.Model.add_metadata('evaluation_results', eval_results)
        
        self.logger.info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ - {split}é›†:")
        for metric, value in results.items():
            self.logger.info(f"  {metric}: {value:.4f}")
        
        return results
    
    def explain(self, nadata, method='importance') -> Dict[str, Any]:
        """
        æ¨¡å‹è§£é‡Š
        
        Args:
            nadata: nadataå¯¹è±¡
            method: è§£é‡Šæ–¹æ³•
            
        Returns:
            è§£é‡Šç»“æœå­—å…¸
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        if method == 'importance':
            try:
                # è·å–åµŒå…¥ç»“æœ
                embeddings = self.predict(nadata)
                
                # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºé‡æ„è¯¯å·®ï¼‰
                feature_importance = self._calculate_feature_importance(nadata)
                
                # æ’åºå¹¶è·å–å‰20ä¸ªé‡è¦ç‰¹å¾
                top_indices = np.argsort(feature_importance)[::-1][:20]
                top_features = [nadata.Var.iloc[i]['Gene'] for i in top_indices]
                top_scores = feature_importance[top_indices]
                
                # æ‰“å°20ä¸ªtop_features
                self.logger.info(f"  - Top 20 é‡è¦åŸºå› :")
                self.logger.info(f"    {'æ’å':<4} {'åŸºå› å':<15} {'é‡è¦æ€§åˆ†æ•°':<12}")
                self.logger.info(f"    {'-'*4} {'-'*15} {'-'*12}")
                for i, (gene, score) in enumerate(zip(top_features, top_scores)):
                    self.logger.info(f"    {i+1:<4} {gene:<15} {score:<12.4f}")
                
                explain_results = {
                    'importance': {
                        'top_features': top_features,
                        'importance_scores': top_scores.tolist(),
                        'embeddings': embeddings.tolist(),
                        'feature_importance': feature_importance.tolist()
                    }
                }
                
                # ä¿å­˜è§£é‡Šç»“æœ
                nadata.uns['nnea_umap_explain'] = explain_results
                
                self.logger.info(f"æ¨¡å‹è§£é‡Šå®Œæˆ:")
                return explain_results
                
            except Exception as e:
                self.logger.error(f"æ¨¡å‹è§£é‡Šå¤±è´¥: {e}")
                return {}
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è§£é‡Šæ–¹æ³•: {method}")
    
    def _calculate_feature_importance(self, nadata) -> np.ndarray:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§
        
        Args:
            nadata: nadataå¯¹è±¡
            
        Returns:
            ç‰¹å¾é‡è¦æ€§æ•°ç»„
        """
        X = nadata.X
        feature_importance = np.zeros(X.shape[1])
        
        # ä½¿ç”¨é‡æ„è¯¯å·®ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
        for i in range(X.shape[1]):
            # åˆ›å»ºæ‰°åŠ¨æ•°æ®
            X_perturbed = X.copy()
            X_perturbed[:, i] = 0  # å°†ç¬¬iä¸ªç‰¹å¾ç½®é›¶
            
            # è®¡ç®—é‡æ„è¯¯å·®
            X_tensor = torch.FloatTensor(X_perturbed).to(self.device)
            with torch.no_grad():
                encoded, decoded = self.model(X_tensor)
                reconstruction_error = F.mse_loss(decoded, X_tensor).item()
            
            feature_importance[i] = reconstruction_error
        
        return feature_importance
    
    def save_model(self, save_path: str) -> None:
        """
        ä¿å­˜æ¨¡å‹çŠ¶æ€
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªæ„å»º")
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'umap_loss_state_dict': self.umap_loss.state_dict(),
            'config': self.config,
            'device': self.device,
            'is_trained': self.is_trained
        }, save_path)
        
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, load_path: str) -> None:
        """
        åŠ è½½æ¨¡å‹çŠ¶æ€
        
        Args:
            load_path: åŠ è½½è·¯å¾„
        """
        if not os.path.exists(load_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
        checkpoint = torch.load(load_path, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹å‚æ•°
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.umap_loss.load_state_dict(checkpoint['umap_loss_state_dict'])
        
        # æ›´æ–°å…¶ä»–å±æ€§
        if 'config' in checkpoint:
            self.config = checkpoint['config']
        if 'is_trained' in checkpoint:
            self.is_trained = checkpoint['is_trained']
        
        self.logger.info(f"æ¨¡å‹å·²ä» {load_path} åŠ è½½")
    
    def plot_umap_results(self, nadata, title: str = "NNEA UMAP Visualization", 
                         figsize: Tuple[int, int] = (10, 8)) -> None:
        """
        å¯è§†åŒ–UMAPç»“æœ
        
        Args:
            nadata: nadataå¯¹è±¡
            title: å›¾è¡¨æ ‡é¢˜
            figsize: å›¾è¡¨å¤§å°
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # è·å–åµŒå…¥ç»“æœ
        embeddings = self.predict(nadata)
        
        # è·å–æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        labels = None
        if hasattr(nadata, 'Meta') and nadata.Meta is not None:
            target_col = self.config.get('dataset', {}).get('target_column', 'target')
            if target_col in nadata.Meta.columns:
                labels = nadata.Meta[target_col].values
        
        # åˆ›å»ºå¯è§†åŒ–
        plt.figure(figsize=figsize)
        
        if labels is not None:
            # æœ‰æ ‡ç­¾çš„æƒ…å†µ
            unique_labels = np.unique(labels)
            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))
            
            for i, label in enumerate(unique_labels):
                mask = labels == label
                plt.scatter(embeddings[mask, 0], embeddings[mask, 1], 
                           c=[colors[i]], label=f'Class {label}', alpha=0.7)
        else:
            # æ— æ ‡ç­¾çš„æƒ…å†µ
            plt.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.7)
        
        plt.title(title)
        plt.xlabel('UMAP1')
        plt.ylabel('UMAP2')
        
        if labels is not None:
            plt.legend()
        
        plt.tight_layout()
        plt.show()
        
        self.logger.info(f"UMAPå¯è§†åŒ–å·²å®Œæˆ: {title}")

    def get_cache_info(self):
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸
        """
        if hasattr(self.umap_loss, 'is_fitted') and self.umap_loss.is_fitted:
            return {
                'pca_fitted': True,
                'nbr_indices_shape': self.umap_loss.nbr_indices.shape if self.umap_loss.nbr_indices is not None else None,
                'pca_components': self.umap_loss.pca.n_components_ if self.umap_loss.pca else 0,
                'original_data_shape': self.umap_loss.original_data.shape if self.umap_loss.original_data is not None else None,
                'global_distances_shape': self.umap_loss.global_distances.shape if self.umap_loss.global_distances is not None else None,
                'smart_negative_sampling': True
            }
        else:
            return {
                'pca_fitted': False,
                'nbr_indices_shape': None,
                'pca_components': 0,
                'original_data_shape': None,
                'global_distances_shape': None,
                'smart_negative_sampling': False
            }
