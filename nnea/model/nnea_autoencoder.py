import os
from typing import Dict, Any, Optional
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.model_selection import train_test_split
from nnea.model.nnea_model import NNEAModel
from nnea.model.base import BaseModel
from sklearn.metrics import mean_squared_error, mean_absolute_error


class NNEADecoder(nn.Module):
    """
    NNEAè§£ç å™¨
    å°†åŸºå› é›†æ½œåœ¨è¡¨ç¤ºé‡æ„ä¸ºåŸå§‹åŸºå› è¡¨è¾¾æ•°æ®
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–NNEAè§£ç å™¨
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        super(NNEADecoder, self).__init__()
        
        # è·å–é…ç½®å‚æ•°
        self.input_dim = config.get('num_genesets', 20)  # åŸºå› é›†æ•°é‡
        self.output_dim = config.get('input_dim', 0)  # åŸå§‹åŸºå› æ•°é‡
        self.hidden_dims = config.get('decoder_hidden_dims', [128, 256])
        self.dropout_rate = config.get('decoder_dropout', 0.3)
        self.activation = config.get('decoder_activation', 'relu')
        
        # æ„å»ºè§£ç å™¨å±‚
        self._build_decoder_layers()
    
    def _build_decoder_layers(self):
        """æ„å»ºè§£ç å™¨å±‚"""
        layers = []
        current_dim = self.input_dim
        
        # æ„å»ºéšè—å±‚
        for hidden_dim in self.hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU() if self.activation == 'relu' else nn.Tanh(),
                nn.Dropout(self.dropout_rate)
            ])
            current_dim = hidden_dim
        
        # è¾“å‡ºå±‚ - é‡æ„åŸå§‹åŸºå› è¡¨è¾¾
        layers.append(nn.Linear(current_dim, self.output_dim))
        
        self.decoder = nn.Sequential(*layers)
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, num_genesets) - åŸºå› é›†æ½œåœ¨è¡¨ç¤º
            
        Returns:
            é‡æ„çš„åŸºå› è¡¨è¾¾å¼ é‡ (batch_size, num_genes)
        """
        return self.decoder(x)


class NNEAAutoencoder(BaseModel):
    """
    NNEAè‡ªç¼–ç å™¨
    å®ç°å¯è§£é‡Šçš„è‡ªç¼–ç å™¨æ¨¡å‹ï¼Œä»¥TrainableGeneSetLayerä¸ºæ ¸å¿ƒ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–NNEAè‡ªç¼–ç å™¨
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        super().__init__(config)
        self.task = 'autoencoder'
        
    def build(self, nadata) -> None:
        """
        æ„å»ºæ¨¡å‹
        
        Args:
            nadata: nadataå¯¹è±¡
        """
        if nadata is None:
            raise ValueError("nadataå¯¹è±¡ä¸èƒ½ä¸ºç©º")
        
        # è·å–è¾“å…¥ç»´åº¦
        if hasattr(nadata, 'X') and nadata.X is not None:
            input_dim = nadata.X.shape[1]  # åŸºå› æ•°é‡
        else:
            raise ValueError("è¡¨è¾¾çŸ©é˜µæœªåŠ è½½")
        
        # è‡ªç¼–ç å™¨çš„è¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦ç›¸åŒ
        output_dim = input_dim
        
        # è·å–nneaé…ç½®éƒ¨åˆ†
        nnea_config = self.config.get('nnea', {})
        
        # å¤„ç†å…ˆéªŒçŸ¥è¯†
        piror_knowledge = None
        use_piror_knowledge = nnea_config.get('piror_knowledge', {}).get('use_piror_knowledge', False)
        if use_piror_knowledge:
            # è·å–åŸºå› åç§°åˆ—è¡¨
            gene_names = None
            if hasattr(nadata, 'Var') and nadata.Var is not None:
                gene_names = nadata.Var['Gene'].tolist()
                
            if gene_names is not None:
                # ä»nnea.ioæ¨¡å—å¯¼å…¥å…ˆéªŒçŸ¥è¯†åŠ è½½å‡½æ•°
                from nnea.io._load import load_piror_knowledge
                piror_knowledge = load_piror_knowledge(self.config, gene_names)
                
                if piror_knowledge is not None:
                    self.logger.info(f"æˆåŠŸåŠ è½½å…ˆéªŒçŸ¥è¯†ï¼Œå½¢çŠ¶: {piror_knowledge.shape}")
                    piror_knowledge = torch.tensor(piror_knowledge, dtype=torch.float32)
                    # ç¡®ä¿å…ˆéªŒçŸ¥è¯†çŸ©é˜µä¸è¾“å…¥ç»´åº¦åŒ¹é…
                    if piror_knowledge.shape[1] != input_dim:
                        self.logger.warning(f"å…ˆéªŒçŸ¥è¯†çŸ©é˜µç»´åº¦ ({piror_knowledge.shape[1]}) ä¸è¾“å…¥ç»´åº¦ ({input_dim}) ä¸åŒ¹é…")
                        # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ›å»ºéšæœºçŸ©é˜µä½œä¸ºå¤‡ç”¨
                        num_genesets = piror_knowledge.shape[0]
                        piror_knowledge = np.random.rand(num_genesets, input_dim)
                        piror_knowledge = (piror_knowledge > 0.8).astype(np.float32)
                else:
                    self.logger.warning("å…ˆéªŒçŸ¥è¯†åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨éšæœºçŸ©é˜µ")
                    num_genesets = nnea_config.get('geneset_layer', {}).get('num_genesets', 20)
                    piror_knowledge = np.random.rand(num_genesets, input_dim)
                    piror_knowledge = (piror_knowledge > 0.8).astype(np.float32)
            else:
                self.logger.warning("æ— æ³•è·å–åŸºå› åç§°åˆ—è¡¨ï¼Œä½¿ç”¨éšæœºçŸ©é˜µ")
                num_genesets = nnea_config.get('geneset_layer', {}).get('num_genesets', 20)
                piror_knowledge = np.random.rand(num_genesets, input_dim)
                piror_knowledge = (piror_knowledge > 0.8).astype(np.float32)
        
        # æ›´æ–°é…ç½®
        self.config['input_dim'] = input_dim
        self.config['output_dim'] = output_dim
        self.config['device'] = str(self.device)  # ç¡®ä¿è®¾å¤‡é…ç½®æ­£ç¡®ä¼ é€’
        
        # æ›´æ–°nneaé…ç½®ä¸­çš„å…ˆéªŒçŸ¥è¯†
        if 'nnea' not in self.config:
            self.config['nnea'] = {}
        if 'piror_knowledge' not in self.config['nnea']:
            self.config['nnea']['piror_knowledge'] = {}
        self.config['nnea']['piror_knowledge']['piror_knowledge'] = piror_knowledge
        
        # åˆ›å»ºencoderï¼ˆNNEAModelï¼‰
        self.encoder = NNEAModel(self.config)
        self.encoder.to(self.device)
        
        # åˆ›å»ºdecoder
        decoder_config = {
            'num_genesets': self.encoder.num_genesets,
            'input_dim': input_dim,
            'decoder_hidden_dims': self.config.get('decoder', {}).get('hidden_dims', [128, 256]),
            'decoder_dropout': self.config.get('decoder', {}).get('dropout', 0.3),
            'decoder_activation': self.config.get('decoder', {}).get('activation', 'relu')
        }
        self.decoder = NNEADecoder(decoder_config)
        self.decoder.to(self.device)
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡å‹ç»„ä»¶éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.encoder, 'geneset_layer'):
            self.encoder.geneset_layer.to(self.device)

        self.logger.info(f"NNEAè‡ªç¼–ç å™¨å·²æ„å»º: è¾“å…¥ç»´åº¦={input_dim}, è¾“å‡ºç»´åº¦={output_dim}")
        num_genesets = nnea_config.get('geneset_layer', {}).get('num_genesets', 20)
        self.logger.info(f"åŸºå› é›†æ•°é‡: {num_genesets}")
        self.logger.info(f"ä½¿ç”¨å…ˆéªŒçŸ¥è¯†: {use_piror_knowledge}")
        self.logger.info(f"è§£ç å™¨éšè—å±‚ç»´åº¦: {decoder_config['decoder_hidden_dims']}")

    def forward(self, x):
        """
        è‡ªç¼–ç å™¨å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, num_genes)
            
        Returns:
            é‡æ„çš„åŸºå› è¡¨è¾¾å¼ é‡ (batch_size, num_genes)
        """
        # Encoder: è¾“å…¥ -> åŸºå› é›†æ½œåœ¨è¡¨ç¤º
        geneset_output = self.encoder.geneset_layer(*self.encoder._prepare_input_for_geneset(x))
        
        # Decoder: åŸºå› é›†æ½œåœ¨è¡¨ç¤º -> é‡æ„è¾“å‡º
        reconstructed = self.decoder(geneset_output)
        
        return reconstructed

    def get_encoder(self):
        """è·å–ç¼–ç å™¨"""
        return self.encoder
    
    def get_decoder(self):
        """è·å–è§£ç å™¨"""
        return self.decoder
    
    def get_latent_representation(self, x):
        """
        è·å–æ½œåœ¨è¡¨ç¤ºï¼ˆåŸºå› é›†è¾“å‡ºï¼‰
        
        Args:
            x: è¾“å…¥å¼ é‡ (batch_size, num_genes)
            
        Returns:
            æ½œåœ¨è¡¨ç¤ºå¼ é‡ (batch_size, num_genesets)
        """
        return self.encoder.geneset_layer(*self.encoder._prepare_input_for_geneset(x))

    def train(self, nadata, verbose: int = 1, max_epochs: Optional[int] = None, continue_training: bool = False, **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            nadata: nadataå¯¹è±¡
            verbose: è¯¦ç»†ç¨‹åº¦
                0=åªæ˜¾ç¤ºè¿›åº¦æ¡
                1=æ˜¾ç¤ºè®­ç»ƒæŸå¤±ã€è®­ç»ƒé‡æ„æŸå¤±ã€éªŒè¯æŸå¤±ã€éªŒè¯é‡æ„æŸå¤±
                2=åœ¨verbose=1åŸºç¡€ä¸Šå¢åŠ æ˜¾ç¤ºMSEã€MAEç­‰è¯„ä¼°æŒ‡æ ‡
            max_epochs: æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„epochs
            continue_training: æ˜¯å¦ç»§ç»­è®­ç»ƒï¼ˆç”¨äºtailorç­–ç•¥ï¼‰
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """

        # è®¾ç½®CUDAè°ƒè¯•ç¯å¢ƒå˜é‡
        if self.device.type == 'cuda':
            import os
            os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
            self.logger.info("å·²å¯ç”¨CUDAåŒæ­¥æ‰§è¡Œæ¨¡å¼ï¼Œæœ‰åŠ©äºè°ƒè¯•CUDAé”™è¯¯")
        
        # å‡†å¤‡æ•°æ®
        X = nadata.X

        # å°†trainæ•°æ®è¿›ä¸€æ­¥åˆ†å‰²ä¸ºtrainå’Œvalidation
        val_size = self.config.get('dataset', {}).get('val_size', 0.2)
        random_state = self.config.get('dataset', {}).get('random_state', 42)

        # ä»trainæ•°æ®ä¸­åˆ†å‰²å‡ºvalidation
        X_train, X_val = train_test_split(
            X, test_size=val_size, random_state=random_state
        )


        # è®­ç»ƒå‚æ•°
        training_config = self.config.get('training', {})
        if max_epochs is None:
            epochs = training_config.get('epochs', 100)
        else:
            epochs = max_epochs
        learning_rate = training_config.get('learning_rate', 0.001)
        batch_size = training_config.get('batch_size', 32)
        reg_weight = training_config.get('regularization_weight', 0.1)
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶æ„å»ºTensorDataset
        X_train_tensor = torch.FloatTensor(X_train)
        
        # æ„å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆè‡ªç¼–ç å™¨çš„è¾“å…¥å’Œè¾“å‡ºç›¸åŒï¼‰
        train_dataset = torch.utils.data.TensorDataset(X_train_tensor)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.logger.info(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X_train={X_train_tensor.shape}")
        
        # æ„å»ºéªŒè¯æ•°æ®é›†ï¼ˆå¦‚æœæœ‰éªŒè¯æ•°æ®ï¼‰
        val_dataset = None
        if X_val is not None:
            X_val_tensor = torch.FloatTensor(X_val)
            val_dataset = torch.utils.data.TensorDataset(X_val_tensor)
            self.logger.info(f"éªŒè¯æ•°æ®å½¢çŠ¶: X_val={X_val_tensor.shape}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0  # è®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        val_loader = None
        if val_dataset is not None:
            val_loader = torch.utils.data.DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0  # è®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
            )
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° - ç°åœ¨éœ€è¦åŒæ—¶ä¼˜åŒ–encoderå’Œdecoder
        all_params = list(self.encoder.parameters()) + list(self.decoder.parameters())
        optimizer = torch.optim.Adam(all_params, lr=learning_rate)
        criterion = nn.MSELoss()  # è‡ªç¼–ç å™¨ä½¿ç”¨MSEæŸå¤±
        
        # æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µ - è®­ç»ƒTrainableGeneSetLayerçš„indicator
        if not continue_training:
            self.logger.info("ğŸ”§ å¼€å§‹æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µ - è®­ç»ƒåŸºå› é›†å±‚æŒ‡ç¤ºçŸ©é˜µ...")
            
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åœ¨åˆå§‹åŒ–é˜¶æ®µå¯ç”¨assist_layer
            if self.encoder.use_assist_in_init:
                self.encoder.set_assist_layer_mode(True)
                self.logger.info("ğŸ“Š åˆå§‹åŒ–é˜¶æ®µï¼šå¯ç”¨è¾…åŠ©å±‚ï¼Œç›´æ¥æ˜ å°„genesetè¾“å‡ºä¸ºé‡æ„ç»“æœ")
            else:
                self.encoder.set_assist_layer_mode(False)
                self.logger.info("ğŸ“Š åˆå§‹åŒ–é˜¶æ®µï¼šä½¿ç”¨æ ‡å‡†æ¨¡å¼ï¼Œä½¿ç”¨focus_layerè¿›è¡Œé‡æ„")
            
            init_results = self._initialize_geneset_layer(train_loader, optimizer, verbose)
            self.logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {init_results}")
            
            # åˆå§‹åŒ–å®Œæˆåï¼Œåˆ‡æ¢åˆ°æ ‡å‡†æ¨¡å¼ï¼ˆä½¿ç”¨focus_layerï¼‰
            self.encoder.set_assist_layer_mode(False)
            self.logger.info("ğŸ”„ åˆå§‹åŒ–å®Œæˆï¼šåˆ‡æ¢åˆ°æ ‡å‡†æ¨¡å¼ï¼Œä½¿ç”¨focus_layerè¿›è¡Œé‡æ„")
            
            # å°†åˆå§‹åŒ–ç»“æœä¿å­˜åˆ°nadata.unsä¸­
            if not hasattr(nadata, 'uns'):
                nadata.uns = {}
            nadata.uns['init_results'] = init_results
            self.logger.info("ğŸ’¾ åˆå§‹åŒ–ç»“æœå·²ä¿å­˜åˆ°nadata.unsä¸­")
        else:
            # ç»§ç»­è®­ç»ƒæ—¶ï¼Œç¡®ä¿ä½¿ç”¨æ ‡å‡†æ¨¡å¼
            self.encoder.set_assist_layer_mode(False)
            self.logger.info("ğŸ”„ ç»§ç»­è®­ç»ƒï¼šä½¿ç”¨æ ‡å‡†æ¨¡å¼ï¼Œä½¿ç”¨focus_layerè¿›è¡Œé‡æ„")
        
        # æ—©åœæœºåˆ¶å‚æ•°
        patience = training_config.get('patience', 10)
        min_delta = 1e-6  # æœ€å°æ”¹å–„é˜ˆå€¼
        
        # æ—©åœå˜é‡åˆå§‹åŒ–
        best_val_loss = float('inf')
        patience_counter = 0
        early_stopped = False
        
        # æ·»åŠ checkpointä¿å­˜ç›¸å…³å˜é‡
        best_encoder_state = None
        best_decoder_state = None
        best_epoch = 0
        outdir = self.config.get('global', {}).get('outdir', 'experiment/test')
        
        # è®­ç»ƒå¾ªç¯
        train_losses = {'loss': [], 'reg_loss': []}
        val_losses = {'loss': [], 'reg_loss': []}
        
        if verbose >= 1:
            if continue_training:
                self.logger.info(f"ç»§ç»­è®­ç»ƒNNEAè‡ªç¼–ç å™¨... (å‰©ä½™{epochs}ä¸ªepoch)")
            else:
                self.logger.info("å¼€å§‹æ­£å¼è®­ç»ƒNNEAè‡ªç¼–ç å™¨...")
            self.logger.info(f"æ—©åœé…ç½®: patience={patience}, min_delta={min_delta}")
            self.logger.info(f"Checkpointä¿å­˜ç›®å½•: {outdir}")
        
        # å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡
        try:
            from tqdm import tqdm
            use_tqdm = True
        except ImportError:
            use_tqdm = False
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼ˆåªæœ‰verbose=0æ—¶æ˜¾ç¤ºï¼‰
        if verbose == 0 and use_tqdm:
            pbar = tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦")
        else:
            pbar = range(epochs)
        
        for epoch in pbar:
            # è®­ç»ƒæ¨¡å¼
            self.encoder.train()
            self.decoder.train()
            epoch_loss = 0.0
            epoch_reg_loss = 0.0
            num_batches = 0
            train_predictions = []
            train_targets = []
            
            # ä½¿ç”¨æ•°æ®åŠ è½½å™¨è¿›è¡Œæ‰¹å¤„ç†è®­ç»ƒ
            for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                
                try:
                    # å‰å‘ä¼ æ’­ - ä½¿ç”¨è‡ªç¼–ç å™¨çš„forwardæ–¹æ³•
                    outputs = self.forward(batch_X)
                    
                    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–æ— ç©·å¤§
                    if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                        self.logger.error(f"Epoch {epoch}, Batch {batch_idx}: æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–æ— ç©·å¤§å€¼")
                        continue
                    
                    loss = criterion(outputs, batch_y)
                    
                    # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦æœ‰æ•ˆ
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.error(f"Epoch {epoch}, Batch {batch_idx}: æŸå¤±å€¼ä¸ºNaNæˆ–æ— ç©·å¤§")
                        continue
                    
                    # æ·»åŠ æ­£åˆ™åŒ–æŸå¤±ï¼ˆæ¥è‡ªencoderï¼‰
                    reg_loss = self.encoder.regularization_loss()
                    total_loss = loss + reg_weight * reg_loss
                    
                    # åå‘ä¼ æ’­
                    total_loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª - åŒæ—¶è£å‰ªencoderå’Œdecoderçš„å‚æ•°
                    all_params = list(self.encoder.parameters()) + list(self.decoder.parameters())
                    torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    epoch_reg_loss += reg_loss.item()
                    num_batches += 1
                    
                    # æ”¶é›†è®­ç»ƒé¢„æµ‹ç»“æœç”¨äºè®¡ç®—æŒ‡æ ‡
                    if verbose >= 2:
                        predictions = outputs.cpu().detach().numpy()
                        targets = batch_y.cpu().detach().numpy()
                        train_predictions.append(predictions)
                        train_targets.append(targets)
                    
                except Exception as e:
                    self.logger.error(f"Epoch {epoch}, Batch {batch_idx}: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    continue
            
            # è®¡ç®—å¹³å‡æŸå¤±
            if num_batches > 0:
                avg_loss = epoch_loss / num_batches
                avg_reg_loss = epoch_reg_loss / num_batches
                train_losses['loss'].append(avg_loss)
                train_losses['reg_loss'].append(avg_reg_loss)
                
                # verbose=1æ—¶æ˜¾ç¤ºè®­ç»ƒæŸå¤±
                if verbose >= 1:
                    self.logger.info(f"Epoch {epoch}: Train Loss={avg_loss:.4f}, Train Reg_Loss={avg_reg_loss:.4f}")
                
                # verbose=2æ—¶è®¡ç®—å¹¶æ˜¾ç¤ºè®­ç»ƒé›†è¯„ä¼°æŒ‡æ ‡
                if verbose >= 2 and train_predictions and train_targets:
                    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
                    all_train_predictions = np.vstack(train_predictions)
                    all_train_targets = np.vstack(train_targets)
                    
                    # è®¡ç®—è®­ç»ƒé›†è¯„ä¼°æŒ‡æ ‡
                    train_metrics = self._calculate_validation_metrics(all_train_predictions, all_train_targets)
                    
                    # æ˜¾ç¤ºè®­ç»ƒé›†è¯„ä¼°æŒ‡æ ‡
                    train_metrics_info = f"Epoch {epoch} Train Metrics: MSE={train_metrics['mse']:.4f}, MAE={train_metrics['mae']:.4f}"
                    self.logger.info(train_metrics_info)
            
            # éªŒè¯é˜¶æ®µ
            if val_loader is not None:
                self.encoder.eval()
                self.decoder.eval()
                val_loss = 0.0
                val_reg_loss = 0.0
                val_num_batches = 0
                val_predictions = []
                val_targets = []
                
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        batch_X = batch_X.to(self.device)
                        batch_y = batch_y.to(self.device)
                        
                        try:
                            outputs = self.forward(batch_X)
                            loss = criterion(outputs, batch_y)
                            reg_loss = self.encoder.regularization_loss()
                            
                            val_loss += loss.item()
                            val_reg_loss += reg_loss.item()
                            val_num_batches += 1
                            
                            # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè®¡ç®—æŒ‡æ ‡
                            if verbose >= 2:
                                predictions = outputs.cpu().detach().numpy()
                                targets = batch_y.cpu().detach().numpy()
                                val_predictions.append(predictions)
                                val_targets.append(targets)
                            
                        except Exception as e:
                            self.logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                            continue
                
                if val_num_batches > 0:
                    avg_val_loss = val_loss / val_num_batches
                    avg_val_reg_loss = val_reg_loss / val_num_batches
                    val_losses['loss'].append(avg_val_loss)
                    val_losses['reg_loss'].append(avg_val_reg_loss)
                    
                    # verbose=1æ—¶æ˜¾ç¤ºéªŒè¯æŸå¤±
                    if verbose >= 1:
                        self.logger.info(f"Epoch {epoch} Validation: Val Loss={avg_val_loss:.4f}, Val Reg_Loss={avg_val_reg_loss:.4f}")
                    
                    # verbose=2æ—¶è®¡ç®—å¹¶æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                    if verbose >= 2 and val_predictions and val_targets:
                        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœ
                        all_predictions = np.vstack(val_predictions)
                        all_targets = np.vstack(val_targets)
                        
                        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                        val_metrics = self._calculate_validation_metrics(all_predictions, all_targets)
                        
                        # æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
                        metrics_info = f"Epoch {epoch} Val Metrics: MSE={val_metrics['mse']:.4f}, MAE={val_metrics['mae']:.4f}"
                        self.logger.info(metrics_info)
                
                # æ—©åœæ£€æŸ¥å’Œcheckpointä¿å­˜
                if val_loader is not None and avg_val_loss is not None:
                    # æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦æ”¹å–„
                    if avg_val_loss < best_val_loss - min_delta:
                        best_val_loss = avg_val_loss
                        best_epoch = epoch
                        patience_counter = 0
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                        best_encoder_state = self.encoder.state_dict().copy()
                        best_decoder_state = self.decoder.state_dict().copy()
                        
                        # ä¿å­˜checkpoint
                        checkpoint_path = os.path.join(outdir, f"checkpoint_epoch_{epoch}.pth")
                        try:
                            torch.save({
                                'epoch': epoch,
                                'encoder_state_dict': best_encoder_state,
                                'decoder_state_dict': best_decoder_state,
                                'optimizer_state_dict': optimizer.state_dict(),
                                'val_loss': best_val_loss,
                                'train_loss': avg_loss if num_batches > 0 else None,
                                'train_reg_loss': avg_reg_loss if num_batches > 0 else None,
                                'val_reg_loss': avg_val_reg_loss
                            }, checkpoint_path)
                            if verbose >= 1:
                                self.logger.info(f"âœ… Epoch {epoch}: éªŒè¯æŸå¤±æ”¹å–„åˆ° {best_val_loss:.4f}")
                                self.logger.info(f"ğŸ’¾ Checkpointå·²ä¿å­˜åˆ°: {checkpoint_path}")
                        except Exception as e:
                            self.logger.error(f"ä¿å­˜checkpointå¤±è´¥: {e}")
                    else:
                        patience_counter += 1
                        if verbose >= 1:
                            self.logger.info(f"âš ï¸ Epoch {epoch}: éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œpatience_counter={patience_counter}/{patience}")
                    
                    # æ£€æŸ¥æ˜¯å¦è§¦å‘æ—©åœ
                    if patience_counter >= patience:
                        early_stopped = True
                        self.logger.info(f"ğŸ›‘ Epoch {epoch}: è§¦å‘æ—©åœï¼éªŒè¯æŸå¤±åœ¨{patience}ä¸ªepochå†…æœªæ”¹å–„")
                        self.logger.info(f"   æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f} (Epoch {best_epoch})")
                        break
        
        # è®­ç»ƒå®Œæˆï¼Œæ¢å¤æœ€ä½³æ¨¡å‹
        if best_encoder_state is not None and best_decoder_state is not None:
            self.encoder.load_state_dict(best_encoder_state)
            self.decoder.load_state_dict(best_decoder_state)
            self.logger.info(f"ğŸ”„ å·²æ¢å¤æœ€ä½³æ¨¡å‹ (Epoch {best_epoch}, Val Loss: {best_val_loss:.4f})")
            
            # ä¿å­˜æœ€ç»ˆçš„æœ€ä½³æ¨¡å‹
            final_best_model_path = os.path.join(outdir, "best_model_final.pth")
            try:
                torch.save({
                    'encoder_state_dict': best_encoder_state,
                    'decoder_state_dict': best_decoder_state
                }, final_best_model_path)
                self.logger.info(f"ğŸ’¾ æœ€ç»ˆæœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {final_best_model_path}")
            except Exception as e:
                self.logger.error(f"ä¿å­˜æœ€ç»ˆæœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
        
        # è®­ç»ƒå®Œæˆ
        self.is_trained = True
        
        # è®°å½•æ—©åœä¿¡æ¯
        if early_stopped:
            self.logger.info(f"ğŸ“Š è®­ç»ƒå› æ—©åœè€Œç»“æŸï¼Œå®é™…è®­ç»ƒäº†{epoch+1}ä¸ªepoch")
        else:
            self.logger.info(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒäº†{epochs}ä¸ªepoch")
        
        # è¿”å›è®­ç»ƒç»“æœ
        results = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'final_train_loss': train_losses['loss'][-1] if train_losses['loss'] else None,
            'final_val_loss': val_losses['loss'][-1] if val_losses['loss'] else None,
            'epochs_trained': epoch + 1 if early_stopped else epochs,
            'early_stopped': early_stopped,
            'best_val_loss': best_val_loss if val_loader is not None else None,
            'best_epoch': best_epoch if val_loader is not None else None,
            'patience_used': patience_counter if early_stopped else 0
        }
        
        # å°†åˆå§‹åŒ–ç»“æœä¹ŸåŒ…å«åœ¨è¿”å›ç»“æœä¸­
        if not continue_training and 'init_results' in locals():
            results['init_results'] = init_results
            # åŒæ—¶ä¿å­˜åˆ°nadata.unsä¸­ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ä¿å­˜çš„è¯ï¼‰
            if not hasattr(nadata, 'uns'):
                nadata.uns = {}
            if 'init_results' not in nadata.uns:
                nadata.uns['init_results'] = init_results
        
        return results

    def _calculate_validation_metrics(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, float]:
        """
        è®¡ç®—éªŒè¯é›†çš„è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ (N, num_genes)
            targets: çœŸå®æ ‡ç­¾ (N, num_genes)
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        try:
            # è®¡ç®—MSEå’ŒMAE
            mse = mean_squared_error(targets.flatten(), predictions.flatten())
            mae = mean_absolute_error(targets.flatten(), predictions.flatten())
            
            return {
                'mse': mse,
                'mae': mae
            }
        except Exception as e:
            self.logger.error(f"è®¡ç®—éªŒè¯æŒ‡æ ‡æ—¶å‡ºç°é”™è¯¯: {e}")
            return {
                'mse': 0.0,
                'mae': 0.0
            }

    def _initialize_geneset_layer(self, train_loader, optimizer, verbose: int = 1) -> Dict[str, Any]:
        """
        åˆå§‹åŒ–åŸºå› é›†å±‚ - è®­ç»ƒindicatorç›´åˆ°æ»¡è¶³æ¡ä»¶
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            verbose: è¯¦ç»†ç¨‹åº¦
            
        Returns:
            åˆå§‹åŒ–ç»“æœå­—å…¸
        """
        self.logger.info("ğŸ”§ å¼€å§‹åŸºå› é›†å±‚åˆå§‹åŒ–...")
        
        # ç¡®è®¤å½“å‰ä½¿ç”¨assist_layeræ¨¡å¼
        if self.encoder.get_assist_layer_mode():
            self.logger.info("ğŸ“Š åˆå§‹åŒ–é˜¶æ®µï¼šä½¿ç”¨è¾…åŠ©å±‚ç›´æ¥æ˜ å°„genesetè¾“å‡ºä¸ºé‡æ„ç»“æœ")
        else:
            self.logger.warning("âš ï¸ åˆå§‹åŒ–é˜¶æ®µï¼šæœªä½¿ç”¨è¾…åŠ©å±‚ï¼Œå»ºè®®åœ¨åˆå§‹åŒ–é˜¶æ®µå¯ç”¨assist_layer")
        
        # è·å–åŸºå› é›†å±‚é…ç½®
        config = self.config.get('nnea', {}).get('geneset_layer', {})
        geneset_threshold = config.get('geneset_threshold', 1e-5)
        max_set_size = config.get('max_set_size', 50)
        init_max_epochs = config.get('init_max_epochs', 100)
        init_patience = config.get('init_patience', 10)
        
        # è·å–åˆå§‹åŒ–é˜¶æ®µçš„æŸå¤±æƒé‡é…ç½®
        init_task_loss_weight = config.get('init_task_loss_weight', 1.0)
        init_reg_loss_weight = config.get('init_reg_loss_weight', 10.0)
        init_total_loss_weight = config.get('init_total_loss_weight', 1.0)
        
        self.logger.info(f"åˆå§‹åŒ–å‚æ•°: geneset_threshold={geneset_threshold}, max_set_size={max_set_size}")
        self.logger.info(f"åˆå§‹åŒ–æŸå¤±æƒé‡: task_loss_weight={init_task_loss_weight}, reg_loss_weight={init_reg_loss_weight}, total_loss_weight={init_total_loss_weight}")
        
        # åˆå§‹åŒ–å˜é‡
        best_condition_count = float('inf')
        patience_counter = 0
        init_epochs = 0
        
        # åˆå§‹åŒ–è®­ç»ƒå¾ªç¯
        for epoch in range(init_max_epochs):
            self.model.train()
            epoch_loss = 0.0
            num_batches = 0
            
            # è®­ç»ƒä¸€ä¸ªepoch
            for batch_X, batch_y in train_loader:
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                
                try:
                    # å‰å‘ä¼ æ’­ - åœ¨åˆå§‹åŒ–é˜¶æ®µï¼Œæˆ‘ä»¬åªè®­ç»ƒencoderçš„genesetå±‚
                    # ä½¿ç”¨encoderçš„assist_layeræ¨¡å¼è¿›è¡Œé‡æ„
                    if self.encoder.get_assist_layer_mode():
                        # ä½¿ç”¨assist_layerç›´æ¥æ˜ å°„
                        geneset_output = self.encoder.geneset_layer(*self.encoder._prepare_input_for_geneset(batch_X))
                        outputs = self.encoder.assist_layer(geneset_output)
                    else:
                        # ä½¿ç”¨å®Œæ•´çš„è‡ªç¼–ç å™¨
                        outputs = self.forward(batch_X)
                    
                    # è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆé‡æ„æŸå¤±ï¼‰
                    task_loss = self._calculate_task_loss(outputs, batch_y)
                    
                    # è®¡ç®—æ­£åˆ™åŒ–æŸå¤±
                    reg_loss = self.encoder.regularization_loss()
                    
                    # è®¡ç®—æ€»æŸå¤±ï¼ˆä½¿ç”¨é…ç½®çš„æƒé‡ï¼‰
                    total_loss = (init_task_loss_weight * task_loss + 
                                init_reg_loss_weight * reg_loss) * init_total_loss_weight
                    
                    # åå‘ä¼ æ’­
                    total_loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª - åœ¨åˆå§‹åŒ–é˜¶æ®µåªè£å‰ªencoderçš„å‚æ•°
                    torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    
                    epoch_loss += reg_loss.item()
                    num_batches += 1
                    
                except Exception as e:
                    self.logger.error(f"åˆå§‹åŒ–Epoch {epoch}, Batch: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    continue
            
            # æ£€æŸ¥åŸºå› é›†æ¡ä»¶
            condition_met = self._check_geneset_condition(geneset_threshold, max_set_size)
            
            if condition_met:
                init_epochs = epoch + 1
                self.logger.info(f"âœ… åŸºå› é›†å±‚åˆå§‹åŒ–å®Œæˆï¼Œåœ¨ç¬¬{init_epochs}ä¸ªepochæ»¡è¶³æ¡ä»¶")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ•°
            if epoch == init_max_epochs - 1:
                self.logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§åˆå§‹åŒ–è½®æ•°({init_max_epochs})ï¼Œå¼ºåˆ¶ç»“æŸåˆå§‹åŒ–")
                init_epochs = init_max_epochs
                break
            
            # æ—©åœæ£€æŸ¥
            current_condition_count = self._count_genesets_above_threshold(geneset_threshold, max_set_size)
            total_gene_sets = self.model.geneset_layer.num_sets if hasattr(self.model, 'geneset_layer') else self.model.gene_set_layer.num_sets
            # åªæœ‰å½“current_condition_countå¼€å§‹å‡å°‘ï¼ˆå³å°äºtotal_gene_setsï¼‰æ—¶æ‰å¯åŠ¨æ—©åœæœºåˆ¶
            if current_condition_count < total_gene_sets:
                if current_condition_count < best_condition_count:
                    best_condition_count = current_condition_count
                    patience_counter = 0
                else:
                    patience_counter += 1
            if patience_counter >= init_patience:
                self.logger.info(f"âš ï¸ åˆå§‹åŒ–æ—©åœï¼Œè¿ç»­{init_patience}ä¸ªepochæœªæ”¹å–„")
                init_epochs = epoch + 1
                break
            
            if verbose >= 2 and (epoch % 20 == 0 or epoch == init_max_epochs - 1):
                condition_count = total_gene_sets - current_condition_count
                self.logger.info(f"åˆå§‹åŒ–Epoch {epoch}: Reg Loss={epoch_loss/num_batches:.4f}, æ»¡è¶³æ¡ä»¶çš„åŸºå› é›†æ•°: {condition_count}/{total_gene_sets}")

        # è¿”å›åˆå§‹åŒ–ç»“æœ
        init_results = {
            'init_epochs': init_epochs,
            'geneset_threshold': geneset_threshold,
            'max_set_size': max_set_size,
            'init_task_loss_weight': init_task_loss_weight,
            'init_reg_loss_weight': init_reg_loss_weight,
            'init_total_loss_weight': init_total_loss_weight,
            'final_condition_met': self._check_geneset_condition(geneset_threshold, max_set_size),
            'final_condition_count': self._count_genesets_above_threshold(geneset_threshold, max_set_size)
        }
        
        return init_results
    
    def _calculate_task_loss(self, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆé‡æ„æŸå¤±ï¼‰
        
        Args:
            outputs: æ¨¡å‹è¾“å‡º
            targets: çœŸå®æ ‡ç­¾
            
        Returns:
            ä»»åŠ¡æŸå¤±
        """
        # ä½¿ç”¨MSEæŸå¤±
        criterion = nn.MSELoss()
        return criterion(outputs, targets)
    
    def _check_geneset_condition(self, geneset_threshold: float, max_set_size: int) -> bool:
        """
        æ£€æŸ¥åŸºå› é›†æ¡ä»¶æ˜¯å¦æ»¡è¶³
        
        Args:
            geneset_threshold: åŸºå› é›†é˜ˆå€¼
            max_set_size: æœ€å¤§åŸºå› é›†å¤§å°
            
        Returns:
            æ˜¯å¦æ»¡è¶³æ¡ä»¶
        """
        try:
            # è·å–åŸºå› é›†å±‚çš„æŒ‡ç¤ºçŸ©é˜µ
            if hasattr(self.encoder, 'geneset_layer'):
                indicators = self.encoder.geneset_layer.get_set_indicators()
            elif hasattr(self.encoder, 'gene_set_layer'):
                indicators = self.encoder.gene_set_layer.get_set_indicators()
            else:
                self.logger.warning("æœªæ‰¾åˆ°åŸºå› é›†å±‚ï¼Œæ— æ³•æ£€æŸ¥æ¡ä»¶")
                return True  # å¦‚æœæ²¡æœ‰åŸºå› é›†å±‚ï¼Œè®¤ä¸ºæ¡ä»¶æ»¡è¶³
            
            # æ£€æŸ¥æ¯ä¸ªåŸºå› é›†
            for i in range(indicators.shape[0]):
                gene_assignments = indicators[i]
                selected_count = torch.sum(gene_assignments >= geneset_threshold).item()
                
                if selected_count >= max_set_size:
                    return False  # æœ‰ä¸€ä¸ªåŸºå› é›†è¶…è¿‡æœ€å¤§å¤§å°ï¼Œæ¡ä»¶ä¸æ»¡è¶³
            
            return True  # æ‰€æœ‰åŸºå› é›†éƒ½æ»¡è¶³æ¡ä»¶
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥åŸºå› é›†æ¡ä»¶æ—¶å‡ºç°é”™è¯¯: {e}")
            return True  # å‡ºé”™æ—¶è®¤ä¸ºæ¡ä»¶æ»¡è¶³
    
    def _count_genesets_above_threshold(self, geneset_threshold: float, max_set_size: int) -> int:
        """
        è®¡ç®—è¶…è¿‡é˜ˆå€¼çš„åŸºå› é›†æ•°é‡
        
        Args:
            geneset_threshold: åŸºå› é›†é˜ˆå€¼
            max_set_size: æœ€å¤§åŸºå› é›†å¤§å°
            
        Returns:
            è¶…è¿‡é˜ˆå€¼çš„åŸºå› é›†æ•°é‡
        """
        try:
            # è·å–åŸºå› é›†å±‚çš„æŒ‡ç¤ºçŸ©é˜µ
            if hasattr(self.encoder, 'geneset_layer'):
                indicators = self.encoder.geneset_layer.get_set_indicators()
            elif hasattr(self.encoder, 'gene_set_layer'):
                indicators = self.encoder.gene_set_layer.get_set_indicators()
            else:
                return 0
            
            count = 0
            for i in range(indicators.shape[0]):
                gene_assignments = indicators[i]
                selected_count = torch.sum(gene_assignments >= geneset_threshold).item()
                
                if selected_count >= max_set_size:
                    count += 1
            
            return count
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—åŸºå› é›†æ•°é‡æ—¶å‡ºç°é”™è¯¯: {e}")
            return 0

    def predict(self, nadata) -> np.ndarray:
        """
        æ¨¡å‹é¢„æµ‹
        
        Args:
            nadata: nadataå¯¹è±¡
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        self.encoder.eval()
        self.decoder.eval()
        with torch.no_grad():
            X = nadata.X
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.forward(X_tensor)
            return outputs.cpu().numpy()
    
    def evaluate(self, nadata, split='test') -> Dict[str, float]:
        """
        æ¨¡å‹è¯„ä¼°
        
        Args:
            nadata: nadataå¯¹è±¡
            split: è¯„ä¼°çš„æ•°æ®é›†åˆ†å‰²
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # è·å–æ•°æ®ç´¢å¼•
        indices = nadata.Model.get_indices(split)
        if indices is None:
            raise ValueError(f"æœªæ‰¾åˆ°{split}é›†çš„ç´¢å¼•")
        
        # æ ¹æ®ç´¢å¼•è·å–æ•°æ®
        X = nadata.X[indices]
        
        # å¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œé¢„æµ‹
        self.encoder.eval()
        self.decoder.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            predictions = self.forward(X_tensor).cpu().numpy()
        
        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(X.flatten(), predictions.flatten())
        mae = mean_absolute_error(X.flatten(), predictions.flatten())
        
        results = {
            'mse': mse,
            'mae': mae
        }
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°Modelå®¹å™¨
        eval_results = nadata.Model.get_metadata('evaluation_results') or {}
        eval_results[split] = results
        nadata.Model.add_metadata('evaluation_results', eval_results)
        
        self.logger.info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ - {split}é›†:")
        for metric, value in results.items():
            self.logger.info(f"  {metric}: {value:.4f}")
        
        return results
    
    def explain(self, nadata, method='importance') -> Dict[str, Any]:
        """
        æ¨¡å‹è§£é‡Š
        
        Args:
            nadata: nadataå¯¹è±¡
            method: è§£é‡Šæ–¹æ³•
            
        Returns:
            è§£é‡Šç»“æœå­—å…¸
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        if method == 'importance':
            try:
                # è·å–åŸºå› é›†åˆ†é…
                geneset_assignments = self.encoder.get_geneset_assignments().detach().cpu().numpy()
                
                # ä½¿ç”¨DeepLIFTè®¡ç®—åŸºå› é›†é‡è¦æ€§
                geneset_importance = self._calculate_geneset_importance_with_deeplift(nadata)
                
                # è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆå ä½ç¬¦ï¼‰
                attention_weights = self.encoder.get_attention_weights().detach().cpu().numpy()
                
                # ç‰¹å¾é‡è¦æ€§ä½¿ç”¨åŸºå› é›†é‡è¦æ€§ä½œä¸ºæ›¿ä»£
                feature_importance = geneset_importance
                
                # è®¡ç®—åŸºå› é‡è¦æ€§ï¼ˆåŸºäºåŸºå› é›†åˆ†é…å’Œé‡è¦æ€§ï¼‰
                gene_importance = np.zeros(self.encoder.input_dim, dtype=np.float32)
                for i in range(self.encoder.num_genesets):
                    # ç¡®ä¿ç»´åº¦åŒ¹é…ï¼šgeneset_assignments[i]æ˜¯åŸºå› å‘é‡ï¼Œgeneset_importance[i]æ˜¯æ ‡é‡
                    gene_importance += geneset_assignments[i].astype(np.float32) 
            except Exception as e:
                self.logger.warning(f"åŸºå› é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
                # ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•
                gene_importance = np.random.rand(self.encoder.input_dim)
                geneset_importance = np.random.rand(self.encoder.num_genesets)
                attention_weights = np.random.rand(self.encoder.num_genesets)
                feature_importance = geneset_importance
            
            # æ’åºå¹¶è·å–å‰20ä¸ªé‡è¦åŸºå› 
            top_indices = np.argsort(gene_importance)[::-1][:20]
            top_genes = [nadata.Var.iloc[i]['Gene'] for i in top_indices]
            top_scores = gene_importance[top_indices]
            
            # æ‰“å°20ä¸ªtop_genes
            self.logger.info(f"  - Top 20 é‡è¦åŸºå› :")
            self.logger.info(f"    {'æ’å':<4} {'åŸºå› å':<15} {'é‡è¦æ€§åˆ†æ•°':<12}")
            self.logger.info(f"    {'-'*4} {'-'*15} {'-'*12}")
            for i, (gene, score) in enumerate(zip(top_genes, top_scores)):
                self.logger.info(f"    {i+1:<4} {gene:<15} {score:<12.4f}")
            
            # åŸºå› é›†ç²¾ç‚¼å’Œæ³¨é‡Š
            genesets_annotated = {}
            
            try:
                # è·å–åŸºå› åç§°åˆ—è¡¨
                gene_names = nadata.Var['Gene'].tolist()
                
                # è·å–é…ç½®å‚æ•°
                nnea_config = self.config.get('nnea', {})
                geneset_config = nnea_config.get('geneset_layer', {})
                min_set_size = geneset_config.get('min_set_size', 10)
                max_set_size = geneset_config.get('max_set_size', 50)
                
                # ç²¾ç‚¼åŸºå› é›†
                from nnea.utils.enrichment import refine_genesets
                # ä»æ¨¡å‹ä¸­è·å–geneset_thresholdå‚æ•°
                geneset_threshold = self.model.geneset_layer.geneset_threshold
                genesets_refined = refine_genesets(
                    geneset_assignments=geneset_assignments,
                    geneset_importance=geneset_importance,
                    gene_names=gene_names,
                    min_set_size=min_set_size,
                    max_set_size=max_set_size,
                    geneset_threshold=geneset_threshold
                )
                
                # å¦‚æœé…ç½®äº†explain_knowledgeï¼Œè¿›è¡Œæ³¨é‡Š
                explain_knowledge_path = nadata.uns.get('explain_knowledge_path')
                if explain_knowledge_path and genesets_refined:
                    from nnea.utils.enrichment import annotate_genesets
                    genesets_annotated = annotate_genesets(
                        genesets=genesets_refined,
                        gmt_path=explain_knowledge_path,
                        pvalueCutoff=0.05
                    )
                    
                    self.logger.info(f"å®ŒæˆåŸºå› é›†æ³¨é‡Šï¼Œæ³¨é‡Šç»“æœæ•°é‡: {len(genesets_annotated)}")
                
            except Exception as e:
                self.logger.warning(f"åŸºå› é›†ç²¾ç‚¼å’Œæ³¨é‡Šå¤±è´¥: {e}")
                # ä½¿ç”¨ç®€åŒ–çš„åŸºå› é›†åˆ›å»ºæ–¹æ³•
                if len(top_genes) >= 10:
                    genesets_refined = [
                        top_genes[:5],  # å‰5ä¸ªåŸºå› 
                        top_genes[5:10]  # ç¬¬6-10ä¸ªåŸºå› 
                    ]
            
            explain_results = {
                'importance': {
                    'top_genes': top_genes,
                    'importance_scores': top_scores.tolist(),
                    'genesets': genesets_annotated,
                    'geneset_importance': geneset_importance.tolist(),
                    'attention_weights': attention_weights.tolist(),
                    'feature_importance': feature_importance.tolist(),
                    'geneset_assignments': geneset_assignments.tolist()
                }
            }
            
            # ä¿å­˜è§£é‡Šç»“æœ
            nadata.uns['nnea_explain'] = explain_results
            
            self.logger.info(f"æ¨¡å‹è§£é‡Šå®Œæˆ:")

            # æŒ‰geneset_importanceé™åºè¾“å‡ºè¯¦ç»†ä¿¡æ¯
            self.logger.info(f"  - åŸºå› é›†é‡è¦æ€§æ’åºç»“æœ:")

            # åˆ›å»ºæ’åºç´¢å¼•
            sorted_indices = np.argsort(geneset_importance.flatten())[::-1]

            # è·å–åŸºå› åç§°åˆ—è¡¨
            gene_names = nadata.Var['Gene'].tolist()

            # è¾“å‡ºè¡¨å¤´
            self.logger.info(f"    {'é‡è¦æ€§åˆ†æ•°':<12} {'åŸºå› é›†Key':<30} {'TopåŸºå› ':<50}")
            self.logger.info(f"    {'-'*12} {'-'*30} {'-'*50}")
            
            # æŒ‰é‡è¦æ€§é™åºè¾“å‡º
            for i, idx in enumerate(sorted_indices):
                if i >= 20:  # åªæ˜¾ç¤ºå‰20ä¸ª
                    remaining = len(sorted_indices) - 20
                    if remaining > 0:
                        self.logger.info(f"    ... è¿˜æœ‰ {remaining} ä¸ªåŸºå› é›†")
                    break
                
                importance_score = geneset_importance.flatten()[idx]
                
                # è·å–å¯¹åº”çš„geneset key
                geneset_key = f"Geneset_{idx}"
                if genesets_annotated and idx < len(genesets_annotated):
                    # è·å–genesets_annotatedçš„é”®
                    keys_list = list(genesets_annotated.keys())
                    if idx < len(keys_list):
                        geneset_key = keys_list[idx]
                
                # è·å–åˆ†é…ç»™è¯¥åŸºå› é›†çš„top genes
                # åŸºäºgeneset_assignmentsçŸ©é˜µï¼Œæ‰¾åˆ°åˆ†é…ç»™è¯¥åŸºå› é›†çš„é‡è¦åŸºå› 
                gene_assignments = geneset_assignments[idx]  # è¯¥åŸºå› é›†çš„åŸºå› åˆ†é…æƒé‡
                top_gene_indices = np.argsort(gene_assignments)[::-1][:5]  # å–å‰5ä¸ªæœ€é‡è¦çš„åŸºå› 
                top_genes_for_geneset = [gene_names[j] for j in top_gene_indices if j < len(gene_names)]
                top_genes_str = ", ".join(top_genes_for_geneset)
                
                self.logger.info(f"    {importance_score:<12.4f} {geneset_key:<30} {top_genes_str:<50}")
            
            return explain_results
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è§£é‡Šæ–¹æ³•: {method}")
    
    def _calculate_geneset_importance_with_deeplift(self, nadata) -> np.ndarray:
        """
        ä½¿ç”¨DeepLIFTè®¡ç®—åŸºå› é›†é‡è¦æ€§
        
        Args:
            nadata: nadataå¯¹è±¡
            
        Returns:
            åŸºå› é›†é‡è¦æ€§æ•°ç»„
        """
        self.model.eval()
        
        # è·å–æ•°æ®
        X = nadata.X
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        # ä¸ºåŸºå› é›†å±‚å‡†å¤‡è¾“å…¥
        R, S = self.encoder._prepare_input_for_geneset(X_tensor)
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ç§¯åˆ†æ¢¯åº¦
        all_ig_scores = []
        
        for i in range(min(100, len(X))):  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜æ•ˆç‡
            # è·å–å•ä¸ªæ ·æœ¬
            R_sample = R[i:i+1]
            S_sample = S[i:i+1]
            
            # è®¡ç®—è¯¥æ ·æœ¬çš„ç§¯åˆ†æ¢¯åº¦
            ig_score = self._integrated_gradients_for_genesets(
                R_sample, S_sample, steps=50
            )
            all_ig_scores.append(ig_score.cpu().numpy())
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§åˆ†æ•°
        avg_ig_scores = np.mean(all_ig_scores, axis=0)
        
        return avg_ig_scores
    
    def _integrated_gradients_for_genesets(self, R, S, target_class=None, baseline=None, steps=50):
        """
        ä½¿ç”¨ç§¯åˆ†æ¢¯åº¦è§£é‡ŠåŸºå› é›†é‡è¦æ€§
        
        Args:
            R: åŸºå› è¡¨è¾¾æ•°æ® (1, num_genes)
            S: åŸºå› æ’åºç´¢å¼• (1, num_genes)
            target_class: è¦è§£é‡Šçš„ç›®æ ‡ç±»åˆ« (é»˜è®¤ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç±»åˆ«)
            baseline: åŸºå› é›†çš„åŸºçº¿å€¼ (é»˜è®¤å…¨é›¶å‘é‡)
            steps: ç§¯åˆ†è·¯å¾„çš„æ’å€¼æ­¥æ•°
            
        Returns:
            ig: åŸºå› é›†é‡è¦æ€§åˆ†æ•° (num_sets,)
        """
        # ç¡®ä¿è¾“å…¥ä¸ºå•æ ·æœ¬
        assert R.shape[0] == 1 and S.shape[0] == 1, "åªæ”¯æŒå•æ ·æœ¬è§£é‡Š"
        
        # è®¡ç®—æ ·æœ¬çš„å¯Œé›†åˆ†æ•° (es_scores)
        with torch.no_grad():
            es_scores = self.encoder.geneset_layer(R, S)  # (1, num_sets)
        
        # ç¡®å®šç›®æ ‡ç±»åˆ«
        if target_class is None:
            with torch.no_grad():
                # ä»Rå’ŒSé‡æ„åŸå§‹è¾“å…¥x
                x = R  # å¯¹äºNNEAåŒ…ä¸­çš„æ¨¡å‹ï¼ŒRå°±æ˜¯åŸå§‹è¾“å…¥
                output = self.forward(x)
                if self.encoder.output_dim == 1:
                    target_class = 0  # äºŒåˆ†ç±»
                else:
                    target_class = torch.argmax(output, dim=1).item()
        
        # è®¾ç½®åŸºçº¿å€¼
        if baseline is None:
            baseline = torch.zeros_like(es_scores)
        
        # ç”Ÿæˆæ’å€¼è·¯å¾„ (stepsä¸ªç‚¹)
        scaled_es_scores = []
        for step in range(1, steps + 1):
            alpha = step / steps
            interpolated = baseline + alpha * (es_scores - baseline)
            scaled_es_scores.append(interpolated)
        
        # å­˜å‚¨æ¢¯åº¦
        gradients = []
        
        # è®¡ç®—æ’å€¼ç‚¹æ¢¯åº¦
        for interp_es in scaled_es_scores:
            interp_es = interp_es.clone().requires_grad_(True)
            
            # æ ¹æ®è¾“å‡ºç»´åº¦å¤„ç†
            if self.encoder.output_dim == 1:
                # äºŒåˆ†ç±»
                logits = self.encoder.focus_layer(interp_es)
                target_logit = logits.squeeze()
            else:
                # å¤šåˆ†ç±»
                logits = self.encoder.focus_layer(interp_es)
                target_logit = logits[0, target_class]
            
            # è®¡ç®—æ¢¯åº¦
            grad = torch.autograd.grad(outputs=target_logit, inputs=interp_es)[0]
            gradients.append(grad.detach())
        
        # æ•´åˆæ¢¯åº¦è®¡ç®—ç§¯åˆ†æ¢¯åº¦
        gradients = torch.stack(gradients)  # (steps, 1, num_sets)
        avg_gradients = torch.mean(gradients, dim=0)  # (1, num_sets)
        ig = (es_scores - baseline) * avg_gradients  # (1, num_sets)
        
        return ig.squeeze(0)  # (num_sets,)
    
    def save_model(self, save_path: str) -> None:
        """
        ä¿å­˜æ¨¡å‹çŠ¶æ€
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        if self.encoder is None or self.decoder is None:
            raise ValueError("æ¨¡å‹æœªæ„å»º")
        
        # ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸
        torch.save({
            'encoder_state_dict': self.encoder.state_dict(),
            'decoder_state_dict': self.decoder.state_dict(),
            'config': self.config,
            'device': self.device,
            'is_trained': self.is_trained
        }, save_path)
        
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, load_path: str) -> None:
        """
        åŠ è½½æ¨¡å‹çŠ¶æ€
        
        Args:
            load_path: åŠ è½½è·¯å¾„
        """
        if not os.path.exists(load_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
        checkpoint = torch.load(load_path, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹å‚æ•°
        if 'encoder_state_dict' in checkpoint:
            self.encoder.load_state_dict(checkpoint['encoder_state_dict'])
        if 'decoder_state_dict' in checkpoint:
            self.decoder.load_state_dict(checkpoint['decoder_state_dict'])
        
        # æ›´æ–°å…¶ä»–å±æ€§
        if 'config' in checkpoint:
            self.config = checkpoint['config']
        if 'is_trained' in checkpoint:
            self.is_trained = checkpoint['is_trained']
        
        self.logger.info(f"æ¨¡å‹å·²ä» {load_path} åŠ è½½")
